# Phase 1.5: ëŒ€ê·œëª¨ ê²€ì¦ ë° Threshold ìµœì í™” ì‘ì—… ì§€ì‹œì„œ

## ğŸ¯ ìˆ˜ì •ëœ ëª©í‘œ

### ê¸°ì¡´ ëª©í‘œ (Phase 1)
- ~~ëª¨ë“  ë¼ë²¨ë§ ê¸‰ë“± í¬ì°©~~ âŒ ë¹„í˜„ì‹¤ì 

### ìƒˆë¡œìš´ ëª©í‘œ (Phase 1.5)
- **ë‹¤ìˆ˜ì˜ ê¸‰ë“±ì„ ë¹ ë¥´ê²Œ í¬ì°©** (Recall 50-70% ëª©í‘œ)
- **FP/h â‰¤ 30-40 ìœ ì§€**
- **ë‹¤ì–‘í•œ í˜•íƒœ/ê°•ë„ ê¸‰ë“±ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥**

**í•µì‹¬ ì¸ì‹**: 
- ëª¨ë“  ê¸‰ë“±ì€ í¬ì°© ë¶ˆê°€ëŠ¥ (ë„ˆë¬´ ë‹¤ì–‘í•¨)
- í•˜ì§€ë§Œ **ìƒë‹¹ìˆ˜(50-70%)ë¥¼ ë¹ ë¥´ê²Œ** ì¡ëŠ” ê²ƒì´ ëª©í‘œ
- ë†“ì¹œ ê¸‰ë“±ì€ Phase 2ì—ì„œ íŒ¨í„´ ë¶„ì„ìœ¼ë¡œ ë³´ì™„

---

## ğŸ“Š ë°ì´í„° ê·œëª¨

### ì´ 12ê°œ íŒŒì¼, 21ê°œ ê¸‰ë“±
- **ê°•í•œ ê¸‰ë“±**: 4ê°œ (19%)
- **ì¤‘ê°„ ê¸‰ë“±**: 9ê°œ (43%)
- **ì•½í•œ ê¸‰ë“±**: 8ê°œ (38%)

### ê¸°ëŒ€ì¹˜ ì„¤ì •
```
í˜„ì¬ ì„¤ì • (Sharp ìµœì í™”):
- ê°•í•œ: 80-100% ì˜ˆìƒ
- ì¤‘ê°„: 50-70% ì˜ˆìƒ
- ì•½í•œ: 20-40% ì˜ˆìƒ

â†’ ì „ì²´ Recall: 50-65% ì˜ˆìƒ
â†’ ëª©í‘œ: 60% ë‹¬ì„± (21ê°œ ì¤‘ 13ê°œ)
```

---

## ğŸ“‹ ì‘ì—… ìˆœì„œ

### Step 1: ëŒ€ê·œëª¨ ê¸‰ë“± ë¼ë²¨ íŒŒì¼ ìƒì„±

```python
# íŒŒì¼: scripts/create_surge_labels.py (ì‹ ê·œ)

"""
12ê°œ íŒŒì¼ 21ê°œ ê¸‰ë“± ë¼ë²¨ë§ ë°ì´í„° ìƒì„±
"""

import pandas as pd
import json
from pathlib import Path

# ê¸‰ë“± ë¼ë²¨ ì •ì˜
surge_labels = {
    "023790_20250901": [
        {"time": "09:55", "duration_min": 3, "strength": "ì¤‘ê°„", "name": "Surge1"},
        {"time": "10:26", "duration_min": 9, "strength": "ì¤‘ê°„", "name": "Surge2"}
    ],
    "023790_20250902": [
        {"time": "09:03", "duration_min": 9, "strength": "ê°•í•œ", "name": "Surge1"}
    ],
    "054540_20250901": [
        {"time": "10:08", "duration_min": 11, "strength": "ê°•í•œ", "name": "Surge1"}
    ],
    "054540_20250902": [
        {"time": "09:45", "duration_min": 3, "strength": "ì•½í•œ", "name": "Surge1"},
        {"time": "09:51", "duration_min": 12, "strength": "ì¤‘ê°„", "name": "Surge2"}
    ],
    "054540_20250903": [
        {"time": "09:20", "duration_min": 5, "strength": "ì•½í•œ", "name": "Surge1"}
    ],
    "362320_20250901": [
        {"time": "13:00", "duration_min": 4, "strength": "ì•½í•œ", "name": "Surge1"}
    ],
    "097230_20250902": [
        {"time": "09:13", "duration_min": 15, "strength": "ê°•í•œ", "name": "Surge1"}
    ],
    "097230_20250903": [
        {"time": "09:13", "duration_min": 4, "strength": "ì•½í•œ", "name": "Surge1"},
        {"time": "09:32", "duration_min": 5, "strength": "ì•½í•œ", "name": "Surge2"}
    ],
    "355690_20250903": [
        {"time": "10:37", "duration_min": 7, "strength": "ì•½í•œ", "name": "Surge1"},
        {"time": "10:49", "duration_min": 8, "strength": "ì¤‘ê°„", "name": "Surge2"}
    ],
    "049470_20250903": [
        {"time": "14:04", "duration_min": 11, "strength": "ì•½í•œ", "name": "Surge1"}
    ],
    "208640_20250903": [
        {"time": "09:42", "duration_min": 2, "strength": "ì¤‘ê°„", "name": "Surge1"}
    ],
    "413630_20250911": [
        {"time": "09:09", "duration_min": 6, "strength": "ì¤‘ê°„", "name": "Surge1"},
        {"time": "10:01", "duration_min": 13, "strength": "ê°•í•œ", "name": "Surge2"},
        {"time": "11:46", "duration_min": 4, "strength": "ì•½í•œ", "name": "Surge3"},
        {"time": "13:29", "duration_min": 8, "strength": "ì¤‘ê°„", "name": "Surge4"},
        {"time": "14:09", "duration_min": 3, "strength": "ì•½í•œ", "name": "Surge5"}
    ]
}

def calculate_surge_timestamps(file_key, surge_info, data_dir="data/raw"):
    """ê¸‰ë“± ì‹œì‘/ì¢…ë£Œ timestamp ê³„ì‚°"""
    
    # íŒŒì¼ëª… ìƒì„±
    stock_code, date = file_key.split('_')
    filename = f"{stock_code}_44indicators_realtime_{date}_clean.csv"
    filepath = Path(data_dir) / filename
    
    if not filepath.exists():
        print(f"âš ï¸ File not found: {filepath}")
        return None
    
    # CSV ë¡œë“œ (ì²« í–‰ë§Œ)
    df = pd.read_csv(filepath, nrows=1)
    first_ts = df['ts'].iloc[0]
    first_dt = pd.to_datetime(first_ts, unit='ms', utc=True).tz_convert('Asia/Seoul')
    
    results = []
    for surge in surge_info:
        hour, minute = map(int, surge['time'].split(':'))
        
        # ê¸‰ë“± ì‹œì‘ ì‹œì 
        surge_start_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
        surge_start_ts = int(surge_start_dt.timestamp() * 1000)
        
        # ê¸‰ë“± ì¢…ë£Œ (duration í›„)
        surge_end_ts = surge_start_ts + (surge['duration_min'] * 60 * 1000)
        
        results.append({
            "file": filename,
            "stock_code": stock_code,
            "surge_name": surge['name'],
            "strength": surge['strength'],
            "start_time": surge['time'],
            "start_ts": surge_start_ts,
            "end_ts": surge_end_ts,
            "duration_min": surge['duration_min']
        })
    
    return results

# ëª¨ë“  ê¸‰ë“± ë¼ë²¨ ìƒì„±
print("="*60)
print("ê¸‰ë“± ë¼ë²¨ ë°ì´í„° ìƒì„±")
print("="*60)

all_labels = []
for file_key, surges in surge_labels.items():
    results = calculate_surge_timestamps(file_key, surges)
    if results:
        all_labels.extend(results)

print(f"\nì´ {len(all_labels)}ê°œ ê¸‰ë“± ë¼ë²¨ ìƒì„±")

# ê°•ë„ë³„ í†µê³„
strength_counts = {}
for label in all_labels:
    strength = label['strength']
    strength_counts[strength] = strength_counts.get(strength, 0) + 1

print(f"\nê°•ë„ë³„ ë¶„í¬:")
for strength, count in sorted(strength_counts.items()):
    print(f"  {strength}: {count}ê°œ ({count/len(all_labels)*100:.1f}%)")

# ì €ì¥
output_path = Path("data/labels/all_surge_labels.json")
output_path.parent.mkdir(parents=True, exist_ok=True)

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(all_labels, f, indent=2, ensure_ascii=False)

print(f"\nì €ì¥: {output_path}")
```

**ì‹¤í–‰**:
```bash
python scripts/create_surge_labels.py
```

---

### Step 2: ë°°ì¹˜ Detection ì‹¤í–‰

```python
# íŒŒì¼: scripts/batch_detection.py (ì‹ ê·œ)

"""
12ê°œ íŒŒì¼ ë°°ì¹˜ Detection
ëª©ì : í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ì „ì²´ íŒŒì¼ ê²€ì¦
"""

import pandas as pd
import json
from pathlib import Path
from onset_detection.src.detection.onset_pipeline import OnsetPipelineDF
from onset_detection.src.features.core_indicators import calculate_core_indicators
from onset_detection.src.config_loader import load_config

# ë¼ë²¨ ë¡œë“œ
with open("data/labels/all_surge_labels.json") as f:
    labels = json.load(f)

# íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
files = list(set([label['file'] for label in labels]))

print("="*60)
print(f"ë°°ì¹˜ Detection ì‹¤í–‰ ({len(files)}ê°œ íŒŒì¼)")
print("="*60)

config = load_config()
results_summary = []

for i, filename in enumerate(files, 1):
    print(f"\n[{i}/{len(files)}] {filename}")
    
    filepath = Path("data/raw") / filename
    if not filepath.exists():
        print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ, ê±´ë„ˆëœ€")
        continue
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)
    features_df = calculate_core_indicators(df)
    
    # Detection ì‹¤í–‰
    pipeline = OnsetPipelineDF(config=config)
    confirmed = pipeline.run_batch(features_df)
    
    # ê²°ê³¼ ì €ì¥
    stock_code = filename.split('_')[0]
    date = filename.split('_')[-2]
    output_file = f"strategy_c_plus_{stock_code}_{date}.jsonl"
    output_path = Path("data/events/batch") / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        for event in confirmed:
            f.write(json.dumps(event, ensure_ascii=False) + '\n')
    
    # í†µê³„
    duration_hours = (df['ts'].max() - df['ts'].min()) / (1000 * 3600)
    fp_per_hour = len(confirmed) / duration_hours
    
    print(f"  Confirmed: {len(confirmed)}ê°œ")
    print(f"  FP/h: {fp_per_hour:.1f}")
    print(f"  ì €ì¥: {output_path}")
    
    results_summary.append({
        "file": filename,
        "confirmed": len(confirmed),
        "fp_per_hour": fp_per_hour,
        "duration_hours": duration_hours
    })

# ì „ì²´ ìš”ì•½
print("\n" + "="*60)
print("ë°°ì¹˜ ì‹¤í–‰ ì™„ë£Œ")
print("="*60)

total_confirmed = sum([r['confirmed'] for r in results_summary])
total_hours = sum([r['duration_hours'] for r in results_summary])
avg_fp_per_hour = total_confirmed / total_hours

print(f"\nì´ íŒŒì¼: {len(results_summary)}ê°œ")
print(f"ì´ Confirmed: {total_confirmed}ê°œ")
print(f"ì´ ì‹œê°„: {total_hours:.1f}ì‹œê°„")
print(f"í‰ê·  FP/h: {avg_fp_per_hour:.1f}")

# ì €ì¥
with open("reports/batch_detection_summary.json", "w") as f:
    json.dump({
        "files": results_summary,
        "summary": {
            "total_files": len(results_summary),
            "total_confirmed": total_confirmed,
            "total_hours": total_hours,
            "avg_fp_per_hour": avg_fp_per_hour
        }
    }, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/batch_detection_summary.json")
```

**ì‹¤í–‰**:
```bash
python scripts/batch_detection.py
```

---

### Step 3: Recall ê³„ì‚° ë° ë¶„ì„

```python
# íŒŒì¼: scripts/calculate_batch_recall.py (ì‹ ê·œ)

"""
21ê°œ ê¸‰ë“±ì— ëŒ€í•œ Recall ê³„ì‚°
"""

import pandas as pd
import json
from pathlib import Path

# ë¼ë²¨ ë¡œë“œ
with open("data/labels/all_surge_labels.json") as f:
    labels = json.load(f)

print("="*60)
print(f"Recall ê³„ì‚° ({len(labels)}ê°œ ê¸‰ë“±)")
print("="*60)

detection_results = []

for label in labels:
    file = label['file']
    stock_code = label['stock_code']
    date = file.split('_')[-2]
    surge_name = label['surge_name']
    strength = label['strength']
    
    start_ts = label['start_ts']
    end_ts = label['end_ts']
    
    # ì´ë²¤íŠ¸ íŒŒì¼ ë¡œë“œ
    event_file = f"strategy_c_plus_{stock_code}_{date}.jsonl"
    event_path = Path("data/events/batch") / event_file
    
    if not event_path.exists():
        print(f"âš ï¸ ì´ë²¤íŠ¸ íŒŒì¼ ì—†ìŒ: {event_path}")
        detection_results.append({
            **label,
            "detected": False,
            "detection_ts": None,
            "latency_s": None
        })
        continue
    
    # ì´ë²¤íŠ¸ ë¡œë“œ
    events = []
    with open(event_path) as f:
        for line in f:
            events.append(json.loads(line))
    
    # ê¸‰ë“± êµ¬ê°„ ë‚´ íƒì§€ ì—¬ë¶€ (Â±30ì´ˆ í—ˆìš©)
    detected_events = [
        e for e in events
        if start_ts - 30000 <= e['ts'] <= end_ts + 30000
    ]
    
    if detected_events:
        # ê°€ì¥ ë¹ ë¥¸ íƒì§€
        first_detection = min(detected_events, key=lambda x: x['ts'])
        detection_ts = first_detection['ts']
        latency_s = (detection_ts - start_ts) / 1000.0
        
        detection_results.append({
            **label,
            "detected": True,
            "detection_ts": detection_ts,
            "latency_s": latency_s,
            "num_alerts": len(detected_events)
        })
        
        print(f"âœ… {file} {surge_name} ({strength}): {latency_s:+.1f}s")
    else:
        detection_results.append({
            **label,
            "detected": False,
            "detection_ts": None,
            "latency_s": None
        })
        
        print(f"âŒ {file} {surge_name} ({strength}): ë¯¸íƒì§€")

# í†µê³„
print("\n" + "="*60)
print("Recall í†µê³„")
print("="*60)

detected = [r for r in detection_results if r['detected']]
total_recall = len(detected) / len(detection_results)

print(f"\nì „ì²´ Recall: {total_recall*100:.1f}% ({len(detected)}/{len(detection_results)})")

# ê°•ë„ë³„
for strength in ["ê°•í•œ", "ì¤‘ê°„", "ì•½í•œ"]:
    strength_total = [r for r in detection_results if r['strength'] == strength]
    strength_detected = [r for r in detected if r['strength'] == strength]
    
    if strength_total:
        strength_recall = len(strength_detected) / len(strength_total)
        print(f"{strength}: {strength_recall*100:.1f}% ({len(strength_detected)}/{len(strength_total)})")

# Latency ë¶„ì„
latencies = [r['latency_s'] for r in detected]
if latencies:
    import numpy as np
    print(f"\nLatency í†µê³„:")
    print(f"  Mean: {np.mean(latencies):.1f}s")
    print(f"  Median: {np.median(latencies):.1f}s")
    print(f"  Min: {np.min(latencies):.1f}s")
    print(f"  Max: {np.max(latencies):.1f}s")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
print("\n" + "="*60)
if total_recall >= 0.60:
    print(f"âœ… ëª©í‘œ ë‹¬ì„±! (Recall {total_recall*100:.1f}% >= 60%)")
else:
    gap = 0.60 - total_recall
    needed = int(gap * len(detection_results))
    print(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ (Recall {total_recall*100:.1f}% < 60%)")
    print(f"   {needed}ê°œ ë” íƒì§€ í•„ìš”")

# ì €ì¥
with open("reports/batch_recall_results.json", "w", encoding='utf-8') as f:
    json.dump({
        "detection_results": detection_results,
        "summary": {
            "total_surges": len(detection_results),
            "detected_surges": len(detected),
            "total_recall": total_recall,
            "recall_by_strength": {
                "ê°•í•œ": len([r for r in detected if r['strength'] == 'ê°•í•œ']) / len([r for r in detection_results if r['strength'] == 'ê°•í•œ']),
                "ì¤‘ê°„": len([r for r in detected if r['strength'] == 'ì¤‘ê°„']) / len([r for r in detection_results if r['strength'] == 'ì¤‘ê°„']),
                "ì•½í•œ": len([r for r in detected if r['strength'] == 'ì•½í•œ']) / len([r for r in detection_results if r['strength'] == 'ì•½í•œ'])
            },
            "latency_stats": {
                "mean": float(np.mean(latencies)) if latencies else None,
                "median": float(np.median(latencies)) if latencies else None
            }
        }
    }, f, indent=2, ensure_ascii=False)

print(f"\nê²°ê³¼ ì €ì¥: reports/batch_recall_results.json")
```

**ì‹¤í–‰**:
```bash
python scripts/calculate_batch_recall.py
```

---

### Step 4: Threshold ì¡°ì • (í•„ìš” ì‹œ)

```python
# íŒŒì¼: scripts/optimize_for_60_percent.py (ì‹ ê·œ)

"""
Recall 60% ë‹¬ì„±ì„ ìœ„í•œ Threshold ìµœì í™”
"""

import json

# Step 3 ê²°ê³¼ ë¡œë“œ
with open("reports/batch_recall_results.json") as f:
    results = json.load(f)

current_recall = results['summary']['total_recall']

print("="*60)
print("Threshold ìµœì í™” ê¶Œì¥ì‚¬í•­")
print("="*60)

print(f"\ní˜„ì¬ Recall: {current_recall*100:.1f}%")
print(f"ëª©í‘œ Recall: 60%")

if current_recall >= 0.60:
    print("\nâœ… ì´ë¯¸ ëª©í‘œ ë‹¬ì„±! ì¶”ê°€ ì¡°ì • ë¶ˆí•„ìš”")
elif current_recall >= 0.50:
    print(f"\nâš ï¸ ëª©í‘œì— ê·¼ì ‘ ({current_recall*100:.1f}%)")
    print("\në¯¸ì„¸ ì¡°ì • ê¶Œì¥:")
    print("""
```yaml
onset:
  speed:
    ret_1s_threshold: 0.0018  # 0.002 â†’ 0.0018 (10% ì™„í™”)
  participation:
    z_vol_threshold: 2.3      # 2.5 â†’ 2.3 (ì•½ê°„ ì™„í™”)
```

ì˜ˆìƒ íš¨ê³¼: Recall +5-10%, FP/h +5-10
""")
else:
    print(f"\nğŸš¨ í° ê²©ì°¨ ({current_recall*100:.1f}%)")
    print("\nëŒ€í­ ì™„í™” í•„ìš”:")
    print("""
```yaml
onset:
  speed:
    ret_1s_threshold: 0.0015  # 0.002 â†’ 0.0015 (25% ì™„í™”)
  participation:
    z_vol_threshold: 2.0      # 2.5 â†’ 2.0 (ëŒ€í­ ì™„í™”)

detection:
  min_axes_required: 2        # 3 â†’ 2
```

ì˜ˆìƒ íš¨ê³¼: Recall +15-20%, FP/h +15-25
""")

# ë†“ì¹œ ê¸‰ë“± ë¶„ì„
missed = [r for r in results['detection_results'] if not r['detected']]

print(f"\në†“ì¹œ ê¸‰ë“± ë¶„ì„ ({len(missed)}ê°œ):")
for m in missed[:5]:  # ì²˜ìŒ 5ê°œë§Œ
    print(f"  {m['file']} {m['surge_name']} ({m['strength']})")
```

**ì‹¤í–‰**:
```bash
python scripts/optimize_for_60_percent.py
```

---

## âœ… ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Step 1: ê¸‰ë“± ë¼ë²¨ íŒŒì¼ ìƒì„±
- [ ] Step 2: 12ê°œ íŒŒì¼ ë°°ì¹˜ Detection
- [ ] Step 3: Recall ê³„ì‚° (ëª©í‘œ: 60%)
- [ ] Step 4: (í•„ìš” ì‹œ) Threshold ì¡°ì • ë° ì¬ì‹¤í–‰

---

## ğŸš€ í•œ ì¤„ ì‹¤í–‰

```bash
python scripts/create_surge_labels.py && \
python scripts/batch_detection.py && \
python scripts/calculate_batch_recall.py && \
python scripts/optimize_for_60_percent.py
```

---

## ğŸ“Œ ì„±ê³µ ê¸°ì¤€ (ìˆ˜ì •ë¨)

### Phase 1.5 ëª©í‘œ
- **Recall â‰¥ 60%** (21ê°œ ì¤‘ 13ê°œ ì´ìƒ)
- **í‰ê·  FP/h â‰¤ 40**
- **Latency Mean â‰¤ 30s**

**ë‹¬ì„± ì‹œ**: Phase 2ë¡œ ì´ë™
**ë¯¸ë‹¬ ì‹œ**: Threshold ë¯¸ì„¸ ì¡°ì • í›„ ì¬ì‹¤í–‰