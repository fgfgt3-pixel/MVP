알겠습니다. 아래는 **Step 2-3 제안안**입니다. Claude Code가 혼동하지 않도록, 반드시 필요한 이유 설명과 최소한의 실행·검증만 포함했습니다.

---

# Step 2-3 제안 (ML 확률 기반 온셋 강도 도입)

## 🎯 목적

* 기존 **룰 기반 온셋 탐지(Δ 기반 + 가격축 필수 + earliest-hit + 연속성)**에
  **ML 확률 스코어(onset_strength)**를 추가하여,
  “룰 ∧ ML” 결합 방식으로 **라벨링 검증**을 가능하게 한다.

---

## 🛠️ 구현 계획

1. **데이터 준비**

   * 입력: 기존 `features.csv` (슬라이딩 윈도우 포함, Step 2-2에서 확장된 120+ 지표)
   * 출력: 학습/평가용 `X, y` 데이터셋

     * `X`: 지표 값들 (정규화/스케일링 필요)

     * `y`: 사용자가 수작업 라벨링한 onset 구간 (0=비급등, 1=온셋)

   > **이유:** ML 모델 학습을 위해서는 레이블이 반드시 필요하며, 기존 룰 기반 신호만으로는 지표 중요도와 기여도를 정량화할 수 없기 때문.

---

2. **모델 구조**

   * **Baseline**: LightGBM / XGBoost (빠른 학습·피처 중요도 추출 용이)

   * 출력: `onset_strength ∈ [0,1]` (확률값)

   > **이유:**
   >
   > * 트리 기반 모델은 비선형 피처 관계를 잘 포착.
   > * 피처 중요도를 바로 확인할 수 있어, 어떤 지표가 온셋에 기여하는지 해석 가능.
   > * 추후 강화학습·딥러닝 등으로 확장 가능하지만, MVP에는 과적합 위험이 적은 트리 계열이 적합.

---

3. **룰+ML 결합**

   * 최종 온셋 판정 조건:

     * 가격축 조건(필수) 충족

     * 거래/마찰축 중 ≥1 충족

     * ML 확률(`onset_strength ≥ θ_ml`) 충족

   > **이유:** 룰 기반만 쓰면 오탐/누락 많음, ML만 쓰면 블랙박스화.
   > 두 방식을 결합해 상호 보완하도록 설계.

---

4. **완료 기준**

   * `confirm_test.py` 실행 시, 룰-only / ML-only / 결합 모델의 탐지율 비교 가능
   * 작은 샘플에서라도 **라벨링 검증이 정상 작동**
   * ML 피처 중요도(Feature Importance) 리포트 생성

---

## ▶️ 실행·검증 (필수만)

1. `train_model.py` 실행 → onset_strength 모델 학습
2. `confirm_test.py --use-ml` 실행 → 룰 vs ML vs 결합 결과 로그 확인
3. 출력 확인:

   * 탐지 이벤트 개수 변화
   * ML 스코어 분포
   * 피처 중요도 Top-N

---

👉 이 Step이 완료되면, **사용자 라벨링 기반 검증**이 가능해집니다.
즉, “어떤 지표가 얼마나 중요한지, ML이 얼마나 구분력을 가지는지”를 수치로 확인할 수 있습니다.