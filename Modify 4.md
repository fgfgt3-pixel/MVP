# Phase 1 íƒ€ì´ë° ë¶ˆì¼ì¹˜ ì›ì¸ ë¶„ì„ ì‘ì—… ì§€ì‹œì„œ (Claude Code ì‹¤í–‰ìš©)

## ğŸš¨ ë¬¸ì œ ìƒí™©
- **023790**: ê¸‰ë“± ì‹œì‘ **ì „(-8.8s)** ë˜ëŠ” **ì§í›„(+9.0s)** íƒì§€ âœ…
- **413630**: ê¸‰ë“± ì‹œì‘ **1.5~2.5ë¶„ í›„** íƒì§€ âŒ
- **ë™ì¼í•œ ì„¤ì •**ì¸ë° **ì™„ì „íˆ ë‹¤ë¥¸ ê²°ê³¼** â†’ ê·¼ë³¸ ì›ì¸ íŒŒì•… í•„ìš”

---

## ğŸ“‹ ì‘ì—… ìˆœì„œ (ì—°ì† ì‹¤í–‰)

### Step 1: ê¸‰ë“± êµ¬ê°„ ë°ì´í„° ìƒì„¸ ë¶„ì„

```python
# íŒŒì¼: scripts/investigate_timing_discrepancy.py (ì‹ ê·œ)

"""
íƒ€ì´ë° ë¶ˆì¼ì¹˜ ì›ì¸ ì¡°ì‚¬
ëª©ì : ì™œ 413630ì€ ëŠë¦°ì§€, 023790ê³¼ ë¬´ì—‡ì´ ë‹¤ë¥¸ì§€ íŒŒì•…
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path

def analyze_surge_characteristics(df, surge_start, surge_end, surge_name):
    """ê¸‰ë“± êµ¬ê°„ì˜ íŠ¹ì„± ë¶„ì„"""
    
    # ê¸‰ë“± êµ¬ê°„ ë°ì´í„°
    surge_df = df[(df['ts'] >= surge_start) & (df['ts'] <= surge_end)].copy()
    
    if surge_df.empty:
        return None
    
    # ê¸‰ë“± ì „ 30ì´ˆ (ë² ì´ìŠ¤ë¼ì¸)
    baseline_df = df[(df['ts'] >= surge_start - 30000) & (df['ts'] < surge_start)].copy()
    
    print(f"\n{'='*60}")
    print(f"{surge_name} íŠ¹ì„± ë¶„ì„")
    print(f"{'='*60}")
    
    # ê¸°ë³¸ í†µê³„
    print(f"\n### ê¸°ë³¸ í†µê³„")
    print(f"Duration: {(surge_end - surge_start)/1000:.1f}ì´ˆ")
    print(f"Total ticks: {len(surge_df)}ê°œ")
    print(f"Price change: {(surge_df['price'].iloc[-1] / surge_df['price'].iloc[0] - 1)*100:+.2f}%")
    
    # ì´ˆë‹¹ í‹± ìˆ˜
    surge_df['second'] = (surge_df['ts'] // 1000)
    ticks_per_sec = surge_df.groupby('second').size()
    
    print(f"\n### Ticks per Second")
    print(f"Mean: {ticks_per_sec.mean():.1f}")
    print(f"Median: {ticks_per_sec.median():.1f}")
    print(f"Min: {ticks_per_sec.min()}")
    print(f"Max: {ticks_per_sec.max()}")
    
    # ret_1s ë¶„ì„
    print(f"\n### ret_1s (Return)")
    print(f"Mean: {surge_df['ret_1s'].mean():.6f}")
    print(f"Median: {surge_df['ret_1s'].median():.6f}")
    print(f"P90: {surge_df['ret_1s'].quantile(0.9):.6f}")
    print(f"Max: {surge_df['ret_1s'].max():.6f}")
    
    # z_vol_1s ë¶„ì„
    if 'z_vol_1s' in surge_df.columns:
        print(f"\n### z_vol_1s (Volume Z-score)")
        print(f"Mean: {surge_df['z_vol_1s'].mean():.2f}")
        print(f"Median: {surge_df['z_vol_1s'].median():.2f}")
        print(f"P90: {surge_df['z_vol_1s'].quantile(0.9):.2f}")
        print(f"Max: {surge_df['z_vol_1s'].max():.2f}")
    
    # Candidate ì¡°ê±´ ì¶©ì¡±ë¥  (ì´ˆê¸° 30ì´ˆ)
    early_surge = surge_df[surge_df['ts'] <= surge_start + 30000]
    
    if not early_surge.empty:
        print(f"\n### ì´ˆê¸° 30ì´ˆ Candidate ì¡°ê±´ ì¶©ì¡±")
        
        # Speed axis
        speed_ok = (early_surge['ret_1s'] > 0.002).sum()
        print(f"Speed (ret_1s > 0.002): {speed_ok}/{len(early_surge)} ({speed_ok/len(early_surge)*100:.1f}%)")
        
        # Participation axis
        if 'z_vol_1s' in early_surge.columns:
            participation_ok = (early_surge['z_vol_1s'] > 2.5).sum()
            print(f"Participation (z_vol > 2.5): {participation_ok}/{len(early_surge)} ({participation_ok/len(early_surge)*100:.1f}%)")
        
        # 3ì¶• ë™ì‹œ ì¶©ì¡±
        if 'z_vol_1s' in early_surge.columns and 'spread' in early_surge.columns:
            spread_baseline = early_surge['spread'].mean() * 1.5
            friction_ok = early_surge['spread'] < spread_baseline * 0.6
            
            all_3_ok = ((early_surge['ret_1s'] > 0.002) & 
                       (early_surge['z_vol_1s'] > 2.5) & 
                       friction_ok).sum()
            print(f"3ì¶• ë™ì‹œ ì¶©ì¡±: {all_3_ok}/{len(early_surge)} ({all_3_ok/len(early_surge)*100:.1f}%)")
    
    # Baseline ëŒ€ë¹„ ë³€í™”
    if not baseline_df.empty:
        print(f"\n### Baseline ëŒ€ë¹„ ë³€í™”")
        
        baseline_tps = baseline_df.groupby(baseline_df['ts'] // 1000).size().mean()
        surge_tps = ticks_per_sec.mean()
        print(f"Ticks/sec: {baseline_tps:.1f} â†’ {surge_tps:.1f} ({(surge_tps/baseline_tps-1)*100:+.1f}%)")
        
        if 'z_vol_1s' in baseline_df.columns and 'z_vol_1s' in surge_df.columns:
            baseline_zvol = baseline_df['z_vol_1s'].median()
            surge_zvol = surge_df['z_vol_1s'].median()
            print(f"z_vol: {baseline_zvol:.2f} â†’ {surge_zvol:.2f} ({surge_zvol - baseline_zvol:+.2f})")
    
    return {
        "duration_s": (surge_end - surge_start) / 1000,
        "total_ticks": len(surge_df),
        "price_change_pct": (surge_df['price'].iloc[-1] / surge_df['price'].iloc[0] - 1) * 100,
        "ticks_per_sec_mean": float(ticks_per_sec.mean()),
        "ret_1s_mean": float(surge_df['ret_1s'].mean()),
        "ret_1s_p90": float(surge_df['ret_1s'].quantile(0.9)),
        "z_vol_mean": float(surge_df['z_vol_1s'].mean()) if 'z_vol_1s' in surge_df.columns else None,
        "early_3axes_rate": float(all_3_ok / len(early_surge)) if not early_surge.empty and all_3_ok else 0
    }

# 023790 ë¶„ì„
print("\n" + "="*60)
print("023790 ê¸‰ë“± ë¶„ì„")
print("="*60)

df_023790 = pd.read_csv("data/raw/023790_44indicators_realtime_20250901_clean.csv")

# Features ê³„ì‚° í•„ìš”
from onset_detection.src.features.core_indicators import calculate_core_indicators
df_023790 = calculate_core_indicators(df_023790)

surges_023790 = [
    {"name": "Surge1", "start": 1756688123304, "end": 1756688123304 + 240000},
    {"name": "Surge2", "start": 1756689969627, "end": 1756689969627 + 240000}
]

results_023790 = []
for surge in surges_023790:
    result = analyze_surge_characteristics(
        df_023790, 
        surge['start'], 
        surge['end'], 
        surge['name']
    )
    if result:
        results_023790.append(result)

# 413630 ë¶„ì„
print("\n" + "="*60)
print("413630 ê¸‰ë“± ë¶„ì„")
print("="*60)

df_413630 = pd.read_csv("data/raw/413630_44indicators_realtime_20250901_clean.csv")
df_413630 = calculate_core_indicators(df_413630)

# ì‹œì‘ ì‹œì  ê³„ì‚°
first_ts = df_413630['ts'].min()
first_dt = pd.to_datetime(first_ts, unit='ms', utc=True).tz_convert('Asia/Seoul')

surge_times = [
    ("09:09", 360000, "Surge1"),  # 6ë¶„
    ("10:01", 780000, "Surge2"),  # 13ë¶„
    ("11:46", 240000, "Surge3"),  # 4ë¶„
    ("13:29", 480000, "Surge4"),  # 8ë¶„
    ("14:09", 180000, "Surge5")   # 3ë¶„
]

surges_413630 = []
for time_str, duration_ms, name in surge_times:
    hour, minute = map(int, time_str.split(':'))
    surge_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
    surge_start = int(surge_dt.timestamp() * 1000)
    surge_end = surge_start + duration_ms
    surges_413630.append({"name": name, "start": surge_start, "end": surge_end})

results_413630 = []
for surge in surges_413630:
    result = analyze_surge_characteristics(
        df_413630,
        surge['start'],
        surge['end'],
        surge['name']
    )
    if result:
        results_413630.append(result)

# ë¹„êµ ë¶„ì„
print("\n" + "="*60)
print("023790 vs 413630 ë¹„êµ")
print("="*60)

print(f"\n### í‰ê·  íŠ¹ì„±")

avg_023790 = {
    "ticks_per_sec": np.mean([r['ticks_per_sec_mean'] for r in results_023790]),
    "ret_1s_p90": np.mean([r['ret_1s_p90'] for r in results_023790]),
    "z_vol_mean": np.mean([r['z_vol_mean'] for r in results_023790 if r['z_vol_mean']]),
    "early_3axes_rate": np.mean([r['early_3axes_rate'] for r in results_023790])
}

avg_413630 = {
    "ticks_per_sec": np.mean([r['ticks_per_sec_mean'] for r in results_413630]),
    "ret_1s_p90": np.mean([r['ret_1s_p90'] for r in results_413630]),
    "z_vol_mean": np.mean([r['z_vol_mean'] for r in results_413630 if r['z_vol_mean']]),
    "early_3axes_rate": np.mean([r['early_3axes_rate'] for r in results_413630])
}

print(f"\n023790 í‰ê· :")
print(f"  Ticks/sec: {avg_023790['ticks_per_sec']:.1f}")
print(f"  ret_1s P90: {avg_023790['ret_1s_p90']:.6f}")
print(f"  z_vol: {avg_023790['z_vol_mean']:.2f}")
print(f"  ì´ˆê¸° 3ì¶• ì¶©ì¡±ë¥ : {avg_023790['early_3axes_rate']*100:.1f}%")

print(f"\n413630 í‰ê· :")
print(f"  Ticks/sec: {avg_413630['ticks_per_sec']:.1f}")
print(f"  ret_1s P90: {avg_413630['ret_1s_p90']:.6f}")
print(f"  z_vol: {avg_413630['z_vol_mean']:.2f}")
print(f"  ì´ˆê¸° 3ì¶• ì¶©ì¡±ë¥ : {avg_413630['early_3axes_rate']*100:.1f}%")

# í•µì‹¬ ë°œê²¬
print("\n" + "="*60)
print("í•µì‹¬ ë°œê²¬ì‚¬í•­")
print("="*60)

if avg_023790['early_3axes_rate'] > avg_413630['early_3axes_rate'] * 2:
    print(f"\nâš ï¸ 413630ì€ ì´ˆê¸° 30ì´ˆì— 3ì¶• ì¶©ì¡±ë¥ ì´ {avg_413630['early_3axes_rate']*100:.1f}%ë¡œ ë§¤ìš° ë‚®ìŒ")
    print(f"   (023790ì€ {avg_023790['early_3axes_rate']*100:.1f}%)")
    print(f"   â†’ ì ì§„ì  ê¸‰ë“± íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì´ˆê¸° íƒì§€ ì–´ë ¤ì›€")

if avg_413630['ticks_per_sec'] < avg_023790['ticks_per_sec'] * 0.7:
    print(f"\nâš ï¸ 413630ì˜ í‹± ë°€ë„ê°€ ë‚®ìŒ ({avg_413630['ticks_per_sec']:.1f} vs {avg_023790['ticks_per_sec']:.1f})")
    print(f"   â†’ Participation axis ì¶©ì¡± ì–´ë ¤ì›€")

if avg_413630['ret_1s_p90'] < 0.002:
    print(f"\nâš ï¸ 413630ì˜ ret_1s P90ì´ ì„ê³„ê°’(0.002) ë¯¸ë§Œ ({avg_413630['ret_1s_p90']:.6f})")
    print(f"   â†’ Speed axis ì¶©ì¡± ì–´ë ¤ì›€")

# ê²°ê³¼ ì €ì¥
summary = {
    "023790": {
        "avg_characteristics": avg_023790,
        "individual_surges": results_023790
    },
    "413630": {
        "avg_characteristics": avg_413630,
        "individual_surges": results_413630
    },
    "comparison": {
        "ticks_ratio": avg_413630['ticks_per_sec'] / avg_023790['ticks_per_sec'],
        "ret_ratio": avg_413630['ret_1s_p90'] / avg_023790['ret_1s_p90'],
        "zvol_ratio": avg_413630['z_vol_mean'] / avg_023790['z_vol_mean'],
        "early_detection_ratio": avg_413630['early_3axes_rate'] / avg_023790['early_3axes_rate']
    }
}

Path("reports").mkdir(exist_ok=True)
with open("reports/timing_discrepancy_analysis.json", "w") as f:
    json.dump(summary, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/timing_discrepancy_analysis.json")
```

**ì‹¤í–‰**:
```bash
python scripts/investigate_timing_discrepancy.py
```

---

### Step 2: ê¸‰ë“± ì‹œì‘ì  ì¬ì •ì˜ í•„ìš”ì„± ê²€í† 

```python
# íŒŒì¼: scripts/verify_surge_start_points.py (ì‹ ê·œ)

"""
ê¸‰ë“± ì‹œì‘ì  ê²€ì¦
ëª©ì : ì‚¬ìš©ìê°€ ì§€ì •í•œ ì‹œì‘ì ì´ ì‹¤ì œ ê¸‰ë“± ì‹œì‘ì¸ì§€ í™•ì¸
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def find_actual_surge_start(df, user_start, window_before=300000, window_after=120000):
    """
    ì‚¬ìš©ì ì§€ì • ì‹œì‘ì  ì „í›„ë¡œ ì‹¤ì œ ê¸‰ë“± ì‹œì‘ íƒìƒ‰
    
    ë°©ë²•:
    1. ticks_per_sec ê¸‰ì¦ ì‹œì 
    2. price ê°€ì† ì‹œì 
    3. z_vol_1s ê¸‰ì¦ ì‹œì 
    """
    
    window_df = df[
        (df['ts'] >= user_start - window_before) & 
        (df['ts'] <= user_start + window_after)
    ].copy()
    
    if window_df.empty:
        return None
    
    # ì´ˆ ë‹¨ìœ„ë¡œ ì§‘ê³„
    window_df['second'] = window_df['ts'] // 1000
    sec_agg = window_df.groupby('second').agg({
        'price': 'last',
        'z_vol_1s': 'mean',
        'ts': 'count'
    }).rename(columns={'ts': 'ticks_count'})
    
    sec_agg['price_pct_change'] = sec_agg['price'].pct_change() * 100
    
    # ë³€í™”ì  íƒì§€
    ticks_mean = sec_agg['ticks_count'].rolling(30).mean()
    ticks_std = sec_agg['ticks_count'].rolling(30).std()
    
    # Ticks ê¸‰ì¦ ì‹œì  (í‰ê·  + 2 std ì´ˆê³¼)
    ticks_surge = sec_agg['ticks_count'] > (ticks_mean + 2 * ticks_std)
    
    # ì²« ë²ˆì§¸ ê¸‰ì¦ ì‹œì 
    if ticks_surge.any():
        first_surge_sec = sec_agg[ticks_surge].index[0]
        first_surge_ts = first_surge_sec * 1000
    else:
        first_surge_ts = user_start
    
    # ì‚¬ìš©ì ì§€ì • ì‹œì‘ vs ì‹¤ì œ ê¸‰ì¦ ì‹œì‘
    difference_s = (first_surge_ts - user_start) / 1000
    
    print(f"\nì‚¬ìš©ì ì§€ì • ì‹œì‘: {pd.to_datetime(user_start, unit='ms', utc=True).tz_convert('Asia/Seoul').strftime('%H:%M:%S')}")
    print(f"Ticks ê¸‰ì¦ ì‹œì‘: {pd.to_datetime(first_surge_ts, unit='ms', utc=True).tz_convert('Asia/Seoul').strftime('%H:%M:%S')}")
    print(f"ì°¨ì´: {difference_s:+.1f}ì´ˆ")
    
    return {
        "user_start": user_start,
        "detected_start": first_surge_ts,
        "difference_s": difference_s
    }

# 413630 ê²€ì¦
print("="*60)
print("413630 ê¸‰ë“± ì‹œì‘ì  ê²€ì¦")
print("="*60)

df_413630 = pd.read_csv("data/raw/413630_44indicators_realtime_20250901_clean.csv")

from onset_detection.src.features.core_indicators import calculate_core_indicators
df_413630 = calculate_core_indicators(df_413630)

first_ts = df_413630['ts'].min()
first_dt = pd.to_datetime(first_ts, unit='ms', utc=True).tz_convert('Asia/Seoul')

surge_times = ["09:09", "10:01", "11:46", "13:29", "14:09"]

results = []
for i, time_str in enumerate(surge_times, 1):
    print(f"\nSurge {i} ({time_str}):")
    
    hour, minute = map(int, time_str.split(':'))
    user_start_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
    user_start = int(user_start_dt.timestamp() * 1000)
    
    result = find_actual_surge_start(df_413630, user_start)
    if result:
        results.append(result)

# í‰ê·  ì°¨ì´
avg_diff = np.mean([r['difference_s'] for r in results])
print(f"\n{'='*60}")
print(f"í‰ê·  ì‹œì‘ì  ì°¨ì´: {avg_diff:+.1f}ì´ˆ")
print(f"{'='*60}")

if abs(avg_diff) > 60:
    print(f"\nâš ï¸ ì‚¬ìš©ì ì§€ì • ì‹œì‘ì ê³¼ ì‹¤ì œ ê¸‰ì¦ ì‹œì‘ì´ {abs(avg_diff):.0f}ì´ˆ ì°¨ì´")
    print(f"   â†’ ê¸‰ë“± 'ì‹œì‘' ì •ì˜ ì¬ê²€í†  í•„ìš”")
    print(f"   â†’ ë˜ëŠ” Candidate thresholdê°€ ì´ ë°ì´í„°ì— ë§ì§€ ì•ŠìŒ")

# ì €ì¥
import json
with open("reports/surge_start_verification.json", "w") as f:
    json.dump(results, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/surge_start_verification.json")
```

**ì‹¤í–‰**:
```bash
python scripts/verify_surge_start_points.py
```

---

### Step 3: ì¢…í•© ì§„ë‹¨ ë° í•´ê²° ë°©ì•ˆ ì œì‹œ

```python
# íŒŒì¼: scripts/diagnose_and_recommend.py (ì‹ ê·œ)

"""
íƒ€ì´ë° ë¶ˆì¼ì¹˜ ì¢…í•© ì§„ë‹¨
ëª©ì : ì›ì¸ íŒŒì•… ë° í•´ê²° ë°©ì•ˆ ì œì‹œ
"""

import json
from pathlib import Path

print("="*60)
print("íƒ€ì´ë° ë¶ˆì¼ì¹˜ ì¢…í•© ì§„ë‹¨")
print("="*60)

# ë¶„ì„ ê²°ê³¼ ë¡œë“œ
with open("reports/timing_discrepancy_analysis.json") as f:
    discrepancy = json.load(f)

with open("reports/surge_start_verification.json") as f:
    verification = json.load(f)

# ì§„ë‹¨
print("\n### ì§„ë‹¨ ê²°ê³¼")

comparison = discrepancy['comparison']

# 1. í‹± ë°€ë„ ì°¨ì´
ticks_ratio = comparison['ticks_ratio']
print(f"\n1. í‹± ë°€ë„ ì°¨ì´")
print(f"   413630 / 023790 = {ticks_ratio:.2f}ë°°")

if ticks_ratio < 0.7:
    print(f"   âš ï¸ 413630ì˜ í‹± ë°€ë„ê°€ {(1-ticks_ratio)*100:.0f}% ë‚®ìŒ")
    print(f"   â†’ ticks_per_sec ê¸°ë°˜ íƒì§€ ì–´ë ¤ì›€")

# 2. Return ì°¨ì´
ret_ratio = comparison['ret_ratio']
print(f"\n2. Return(ret_1s) ì°¨ì´")
print(f"   413630 / 023790 = {ret_ratio:.2f}ë°°")

if ret_ratio < 0.8:
    print(f"   âš ï¸ 413630ì˜ ìˆ˜ìµë¥ ì´ {(1-ret_ratio)*100:.0f}% ë‚®ìŒ")
    print(f"   â†’ Speed axis ì¶©ì¡± ì–´ë ¤ì›€")

# 3. ì´ˆê¸° 3ì¶• ì¶©ì¡±ë¥ 
early_detection_ratio = comparison['early_detection_ratio']
print(f"\n3. ì´ˆê¸° 30ì´ˆ 3ì¶• ì¶©ì¡±ë¥ ")
print(f"   413630 / 023790 = {early_detection_ratio:.2f}ë°°")

if early_detection_ratio < 0.3:
    print(f"   ğŸš¨ 413630ì€ ì´ˆê¸°ì— 3ì¶• ê±°ì˜ ì¶©ì¡± ëª»í•¨ (í•µì‹¬ ë¬¸ì œ!)")
    print(f"   â†’ ì ì§„ì  ê¸‰ë“± íŠ¹ì„±")

# 4. ì‹œì‘ì  ì •ì˜ ì°¨ì´
avg_start_diff = sum([r['difference_s'] for r in verification]) / len(verification)
print(f"\n4. ê¸‰ë“± ì‹œì‘ì  ê²€ì¦")
print(f"   ì‚¬ìš©ì ì§€ì • vs ì‹¤ì œ ê¸‰ì¦: í‰ê·  {avg_start_diff:+.1f}ì´ˆ")

if abs(avg_start_diff) > 60:
    print(f"   âš ï¸ ì‹œì‘ì  ì •ì˜ì— 1ë¶„ ì´ìƒ ì°¨ì´")
    print(f"   â†’ 'ê¸‰ë“± ì‹œì‘'ì˜ ì •ì˜ê°€ ëª¨í˜¸")

# ê·¼ë³¸ ì›ì¸
print("\n" + "="*60)
print("ê·¼ë³¸ ì›ì¸")
print("="*60)

print("""
413630ì´ ëŠë¦° ì´ìœ :

1. **ì ì§„ì  ê¸‰ë“± íŠ¹ì„±**
   - 023790: ê¸‰ê²©í•œ ê¸‰ë“± (ì´ˆë°˜ë¶€í„° ê°•í•œ ì‹ í˜¸)
   - 413630: ì ì§„ì  ìƒìŠ¹ (ì´ˆë°˜ ì‹ í˜¸ ì•½í•¨)
   
2. **í˜„ì¬ Thresholdì˜ í•œê³„**
   - ret_1s > 0.002: ê¸‰ê²©í•œ ê¸‰ë“±ì— ìµœì í™”
   - z_vol > 2.5: ë†’ì€ ê±°ë˜ëŸ‰ ê¸‰ì¦ ìš”êµ¬
   - 3ì¶• ë™ì‹œ ì¶©ì¡±: ì ì§„ì  ê¸‰ë“±ì€ ì´ˆê¸° ì¶©ì¡± ì–´ë ¤ì›€

3. **ê¸‰ë“± ì •ì˜ì˜ ëª¨í˜¸ì„±**
   - "09:09 ì‹œì‘"ì´ ì‹¤ì œ ê¸‰ì¦ ì‹œì‘ì¸ì§€ ë¶ˆëª…í™•
   - ì ì§„ì  ì „í™˜ êµ¬ê°„ì„ 'ì‹œì‘ì 'ìœ¼ë¡œ ë³´ê¸° ì–´ë ¤ì›€
""")

# í•´ê²° ë°©ì•ˆ
print("\n" + "="*60)
print("í•´ê²° ë°©ì•ˆ")
print("="*60)

print("""
### ì˜µì…˜ A: Threshold ì™„í™” (ì ì§„ì  ê¸‰ë“± í¬ì°©)

**ë³€ê²½**:
```yaml
onset:
  speed:
    ret_1s_threshold: 0.0015  # 0.002 â†’ 0.0015
  participation:
    z_vol_threshold: 2.0      # 2.5 â†’ 2.0
  friction:
    spread_narrowing_pct: 0.7 # 0.6 â†’ 0.7

detection:
  min_axes_required: 2        # 3 â†’ 2 (ì™„í™”)
```

**ì˜ˆìƒ íš¨ê³¼**:
- 413630 Recall í–¥ìƒ (40% â†’ 60-80%)
- FP/h ì¦ê°€ (3.2 â†’ 15-25)
- 023790 Recall ìœ ì§€ (100%)
- 023790 FP/h ì¦ê°€ (20.1 â†’ 40-50)

**íŠ¸ë ˆì´ë“œì˜¤í”„**: FP ì¦ê°€ vs Recall í–¥ìƒ

---

### ì˜µì…˜ B: ë“€ì–¼ ì „ëµ (ê¸‰ë“± íƒ€ì…ë³„ ë¶„ë¦¬)

**ê°œë…**:
- **ê¸‰ê²©í•œ ê¸‰ë“±**: í˜„ì¬ ì„¤ì • (ì—„ê²©)
- **ì ì§„ì  ê¸‰ë“±**: ì™„í™”ëœ ì„¤ì •

**êµ¬í˜„**:
```python
# candidate_detector.pyì— ë“€ì–¼ ëª¨ë“œ ì¶”ê°€
if gradual_mode:
    # ì™„í™”ëœ threshold
else:
    # í˜„ì¬ threshold
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë‘ íƒ€ì… ëª¨ë‘ í¬ì°©
- ë³µì¡ë„ ì¦ê°€
- Phase 2ì—ì„œ ê°•ë„ ë¶„ë¥˜ì™€ í†µí•© ê°€ëŠ¥

---

### ì˜µì…˜ C: í˜„ì¬ ì„¤ì • ìœ ì§€ + ê¸‰ë“± ì¬ì •ì˜

**ì ‘ê·¼**:
1. 413630ì˜ "ì‹œì‘ì "ì„ ì¬ê²€í† 
2. Ticks ê¸‰ì¦ ì‹œì ì„ ì‹¤ì œ ì‹œì‘ìœ¼ë¡œ ì¬ì •ì˜
3. í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ì¬ì¸¡ì •

**ì¥ì **:
- ì„¤ì • ë³€ê²½ ì—†ìŒ
- Recall í–¥ìƒ ê°€ëŠ¥

**ë‹¨ì **:
- ìˆ˜ì‘ì—… ì¬ì •ì˜ í•„ìš”
- ê·¼ë³¸ í•´ê²° ì•„ë‹˜

---

### ê¶Œì¥: ì˜µì…˜ A (Threshold ì™„í™”)

**ì´ìœ **:
1. ì ì§„ì  ê¸‰ë“±ë„ ì‹¤ì œ ê¸‰ë“±ì„
2. FP ì¦ê°€ëŠ” Phase 2ì—ì„œ í•„í„°ë§ ê°€ëŠ¥
3. Recall ìš°ì„  ì›ì¹™ ìœ ì§€
4. êµ¬í˜„ ê°„ë‹¨

**ë‹¤ìŒ ë‹¨ê³„**:
1. Threshold ì™„í™” ì ìš©
2. ë“€ì–¼ íŒŒì¼ ì¬ì¸¡ì •
3. FP/hì™€ Recall ê· í˜•ì  í™•ì¸
4. ëª©í‘œ ë‹¬ì„± ì‹œ Phase 1 ì¢…ë£Œ
""")

# ì €ì¥
diagnosis = {
    "root_cause": {
        "gradual_surge_characteristics": True,
        "threshold_optimized_for_sharp_surges": True,
        "start_point_ambiguity": abs(avg_start_diff) > 60
    },
    "metrics": {
        "ticks_ratio": ticks_ratio,
        "ret_ratio": ret_ratio,
        "early_detection_ratio": early_detection_ratio,
        "start_diff_avg": avg_start_diff
    },
    "recommended_option": "A",
    "next_actions": [
        "Apply relaxed thresholds",
        "Re-run detection on both files",
        "Measure Recall and FP/h",
        "Decide Phase 1 completion"
    ]
}

with open("reports/timing_diagnosis_and_recommendation.json", "w") as f:
    json.dump(diagnosis, f, indent=2)

print(f"\nì§„ë‹¨ ê²°ê³¼ ì €ì¥: reports/timing_diagnosis_and_recommendation.json")
```

**ì‹¤í–‰**:
```bash
python scripts/diagnose_and_recommend.py
```

---

## âœ… ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Step 1: ê¸‰ë“± êµ¬ê°„ ìƒì„¸ ë¶„ì„ ì™„ë£Œ
- [ ] Step 2: ê¸‰ë“± ì‹œì‘ì  ê²€ì¦ ì™„ë£Œ
- [ ] Step 3: ì¢…í•© ì§„ë‹¨ ë° ê¶Œì¥ ë°©ì•ˆ í™•ì¸

---

## ğŸš€ í•œ ì¤„ ì‹¤í–‰

```bash
python scripts/investigate_timing_discrepancy.py && \
python scripts/verify_surge_start_points.py && \
python scripts/diagnose_and_recommend.py && \
cat reports/timing_diagnosis_and_recommendation.json
```

---

## ğŸ“Œ ì˜ˆìƒë˜ëŠ” ê²°ê³¼

### ê°€ì„¤ 1: ì ì§„ì  ê¸‰ë“± íŠ¹ì„±
- 413630ì€ ì´ˆê¸° ì‹ í˜¸ê°€ ì•½í•¨
- 3ì¶• ë™ì‹œ ì¶©ì¡±ì´ 1-2ë¶„ í›„ì—ì•¼ ê°€ëŠ¥
- â†’ Threshold ì™„í™” í•„ìš”

### ê°€ì„¤ 2: í‹± ë°€ë„ ì°¨ì´
- 413630ì˜ í‹±ì´ ì ìŒ
- ticks_per_sec ê¸°ì¤€ ë¯¸ë‹¬
- â†’ Participation ì„ê³„ ë‚®ì¶¤

### ê°€ì„¤ 3: ì‹œì‘ì  ì •ì˜ ì˜¤ë¥˜
- ì‚¬ìš©ì ì§€ì • "ì‹œì‘"ì´ ì‹¤ì œ ê¸‰ì¦ ì´ì „
- 1-2ë¶„ ì„ í–‰ ì‹œì‘ì 
- â†’ ì¬ì •ì˜ í•„ìš”

**ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ì¡°ì¹˜ ê²°ì •**