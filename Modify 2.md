# ì „ëµ C+ ì¶”ê°€ ìµœì í™” ì‘ì—… ì§€ì‹œì„œ (Claude Code ì‹¤í–‰ìš©)

## ğŸ¯ í˜„ì¬ ìƒí™©
- **FP/h: 66.5** (ëª©í‘œ 30ì˜ 2.2ë°° ì´ˆê³¼)
- **Recall: 100%** (ëª©í‘œ ë‹¬ì„±)
- **ê°œì„  ì—¬ì§€**: FPë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì—¬ì•¼ ëª©í‘œ ë‹¬ì„±

---

## ğŸ“‹ ì‘ì—… ìˆœì„œ (ì—°ì† ì‹¤í–‰)

### Step 1: FP ë¶„í¬ ìƒì„¸ ë¶„ì„
**ëª©ì **: 66.5ê°œì˜ FPê°€ ì–´ë””ì„œ ì£¼ë¡œ ë°œìƒí•˜ëŠ”ì§€ íŒŒì•…

```python
# íŒŒì¼: scripts/analyze_fp_distribution.py (ì‹ ê·œ ìƒì„±)

"""
FP(False Positive) ë¶„í¬ ë¶„ì„
ëª©ì : FPê°€ ë°œìƒí•˜ëŠ” ì‹œê°„ëŒ€/íŒ¨í„´/íŠ¹ì„± íŒŒì•…
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
import matplotlib.pyplot as plt

# ì´ë²¤íŠ¸ ë¡œë“œ
events_path = "data/events/strategy_c_results.jsonl"
events = []
with open(events_path, 'r') as f:
    for line in f:
        events.append(json.loads(line))

confirmed = [e for e in events if e['event_type'] == 'onset_confirmed']

# ë°ì´í„° ê¸°ê°„
data_path = "data/raw/023790_44indicators_realtime_20250901_clean.csv"
df = pd.read_csv(data_path)

# ì•Œë ¤ì§„ ê¸‰ë“± êµ¬ê°„ ì •ì˜ (Â±30ì´ˆ)
surge1_start = 1756688123304
surge1_end = surge1_start + 120000  # +2ë¶„
surge1_window_start = surge1_start - 30000
surge1_window_end = surge1_end + 30000

surge2_start = 1756689969627
surge2_end = surge2_start + 120000
surge2_window_start = surge2_start - 30000
surge2_window_end = surge2_end + 30000

# TP vs FP ë¶„ë¥˜
tp_events = []
fp_events = []

for e in confirmed:
    ts = e['ts']
    if (surge1_window_start <= ts <= surge1_window_end) or \
       (surge2_window_start <= ts <= surge2_window_end):
        tp_events.append(e)
    else:
        fp_events.append(e)

print("=" * 60)
print("FP ë¶„í¬ ìƒì„¸ ë¶„ì„")
print("=" * 60)
print(f"\nTotal Confirmed: {len(confirmed)}")
print(f"True Positives (TP): {len(tp_events)}")
print(f"False Positives (FP): {len(fp_events)}")
print(f"FP Rate: {len(fp_events)/len(confirmed)*100:.1f}%")

# FP ì‹œê°„ëŒ€ ë¶„ì„
fp_df = pd.DataFrame(fp_events)
if not fp_df.empty:
    fp_df['datetime'] = pd.to_datetime(fp_df['ts'], unit='ms', utc=True).dt.tz_convert('Asia/Seoul')
    fp_df['hour'] = fp_df['datetime'].dt.hour
    fp_df['minute'] = fp_df['datetime'].dt.minute
    
    print("\n### FP ì‹œê°„ëŒ€ ë¶„í¬ (ì‹œê°„ë³„)")
    hour_dist = fp_df['hour'].value_counts().sort_index()
    for hour, count in hour_dist.items():
        print(f"  {hour:02d}ì‹œ: {count}ê°œ")
    
    # ì¥ ì´ˆë°˜ vs í›„ë°˜
    morning = len(fp_df[fp_df['hour'] < 12])
    afternoon = len(fp_df[fp_df['hour'] >= 12])
    print(f"\nì˜¤ì „ (09-12ì‹œ): {morning}ê°œ ({morning/len(fp_df)*100:.1f}%)")
    print(f"ì˜¤í›„ (12-15ì‹œ): {afternoon}ê°œ ({afternoon/len(fp_df)*100:.1f}%)")

# FP íŠ¹ì„± ë¶„ì„ (evidence)
print("\n### FP íŠ¹ì„± ë¶„ì„")

# Axes ë¶„í¬
axes_counts = {'price': 0, 'volume': 0, 'friction': 0}
onset_strengths = []

for e in fp_events:
    evidence = e.get('evidence', {})
    axes = evidence.get('axes', [])
    for axis in axes:
        if axis in axes_counts:
            axes_counts[axis] += 1
    
    strength = evidence.get('onset_strength', 0)
    onset_strengths.append(strength)

print("Axes ì¶œí˜„ ë¹ˆë„:")
for axis, count in axes_counts.items():
    print(f"  {axis}: {count}ê°œ ({count/len(fp_events)*100:.1f}%)")

print(f"\nOnset Strength í†µê³„:")
print(f"  Mean: {np.mean(onset_strengths):.3f}")
print(f"  Median: {np.median(onset_strengths):.3f}")
print(f"  Min: {np.min(onset_strengths):.3f}")
print(f"  Max: {np.max(onset_strengths):.3f}")
print(f"  P25: {np.percentile(onset_strengths, 25):.3f}")

# FP êµ°ì§‘ ë¶„ì„ (ì—°ì† ë°œìƒ ì—¬ë¶€)
print("\n### FP êµ°ì§‘ ë¶„ì„")
fp_timestamps = sorted([e['ts'] for e in fp_events])
clusters = []
current_cluster = [fp_timestamps[0]]

for i in range(1, len(fp_timestamps)):
    # 5ë¶„(300ì´ˆ) ì´ë‚´ë©´ ê°™ì€ êµ°ì§‘
    if (fp_timestamps[i] - current_cluster[-1]) <= 300000:
        current_cluster.append(fp_timestamps[i])
    else:
        clusters.append(current_cluster)
        current_cluster = [fp_timestamps[i]]
clusters.append(current_cluster)

print(f"ì´ êµ°ì§‘ ìˆ˜: {len(clusters)}")
print(f"í‰ê·  êµ°ì§‘ í¬ê¸°: {np.mean([len(c) for c in clusters]):.1f}ê°œ")
print(f"ìµœëŒ€ êµ°ì§‘ í¬ê¸°: {max([len(c) for c in clusters])}ê°œ")

# í° êµ°ì§‘ (5ê°œ ì´ìƒ) ë¶„ì„
large_clusters = [c for c in clusters if len(c) >= 5]
if large_clusters:
    print(f"\ní° êµ°ì§‘ (â‰¥5ê°œ): {len(large_clusters)}ê°œ")
    for i, cluster in enumerate(large_clusters[:5], 1):
        start_dt = pd.to_datetime(cluster[0], unit='ms', utc=True).tz_convert('Asia/Seoul')
        print(f"  êµ°ì§‘ {i}: {len(cluster)}ê°œ, ì‹œì‘={start_dt.strftime('%H:%M:%S')}")

# ê²°ê³¼ ì €ì¥
summary = {
    "total_confirmed": len(confirmed),
    "true_positives": len(tp_events),
    "false_positives": len(fp_events),
    "fp_rate": len(fp_events) / len(confirmed),
    "fp_by_hour": hour_dist.to_dict() if not fp_df.empty else {},
    "fp_morning": morning if not fp_df.empty else 0,
    "fp_afternoon": afternoon if not fp_df.empty else 0,
    "axes_distribution": axes_counts,
    "onset_strength_stats": {
        "mean": float(np.mean(onset_strengths)),
        "median": float(np.median(onset_strengths)),
        "p25": float(np.percentile(onset_strengths, 25))
    },
    "clusters": {
        "total": len(clusters),
        "avg_size": float(np.mean([len(c) for c in clusters])),
        "large_clusters": len(large_clusters)
    }
}

Path("reports").mkdir(exist_ok=True)
with open("reports/fp_distribution_analysis.json", "w") as f:
    json.dump(summary, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/fp_distribution_analysis.json")

# í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì œì‹œ
print("\n" + "=" * 60)
print("í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
print("=" * 60)

if len(large_clusters) > 0:
    print("âš ï¸ FPê°€ êµ°ì§‘ìœ¼ë¡œ ë°œìƒ â†’ Refractory ì—°ì¥ íš¨ê³¼ì ")
    print(f"   ì œì•ˆ: refractory_së¥¼ 45-60ì´ˆë¡œ ì¦ê°€")

if morning > afternoon * 1.5:
    print("âš ï¸ ì˜¤ì „ì— FP ì§‘ì¤‘ â†’ ì¥ì´ˆë°˜ ì„ê³„ ê°•í™” í•„ìš”")
    print(f"   ì œì•ˆ: 09-11ì‹œ êµ¬ê°„ threshold ìƒí–¥")

if np.median(onset_strengths) < 0.7:
    print("âš ï¸ ì•½í•œ ì‹ í˜¸ê°€ ë§ìŒ â†’ onset_strength ì„ê³„ ì¶”ê°€")
    print(f"   ì œì•ˆ: onset_strength â‰¥ 0.7 ì¡°ê±´ ì¶”ê°€")

if axes_counts['friction'] < len(fp_events) * 0.3:
    print("âš ï¸ Friction axis ê¸°ì—¬ ë‚®ìŒ â†’ min_axes=3 ìœ ì§€ ê¶Œì¥")
```

**ì‹¤í–‰**:
```bash
python scripts/analyze_fp_distribution.py
```

---

### Step 2: ìµœì í™” ì „ëµ ì„ íƒ
**ëª©ì **: Step 1 ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì „ëµ ê²°ì •

**ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì „ëµ**:

#### ì „ëµ 2-A: **Refractory ì—°ì¥** (FP êµ°ì§‘ ë°œìƒ ì‹œ)
```yaml
# config/onset_default.yaml
refractory:
  duration_s: 45  # 30 â†’ 45ì´ˆ
```

#### ì „ëµ 2-B: **Onset Strength ì„ê³„ ì¶”ê°€** (ì•½í•œ ì‹ í˜¸ ë§ì„ ì‹œ)
```python
# src/detection/confirm_detector.py ìˆ˜ì •
# _check_delta_confirmation() ë‚´ë¶€ì— ì¶”ê°€

# ê¸°ì¡´ ì¡°ê±´ ë’¤ì— ì¶”ê°€
if onset_strength < 0.7:
    return {
        "confirmed": False,
        "satisfied_axes": [],
        "onset_strength": onset_strength,
        # ...
    }
```

#### ì „ëµ 2-C: **Persistent_n ì¦ê°€** (ì¼ë°˜ì  ì ‘ê·¼)
```yaml
# config/onset_default.yaml
confirm:
  persistent_n: 25  # 20 â†’ 25 (2.5ì´ˆë¶„)
```

#### ì „ëµ 2-D: **ë³µí•© ì „ëµ** (ê°€ì¥ ì•ˆì „)
- Refractory: 30 â†’ 45ì´ˆ
- Persistent_n: 20 â†’ 22
- Onset_strength â‰¥ 0.67 ì¶”ê°€

---

### Step 3: ì„ íƒëœ ì „ëµ ì ìš© ë° ì¬ì‹¤í–‰

**íŒŒì¼**: `scripts/apply_optimization_strategy.py` (ì‹ ê·œ)

```python
"""
ìµœì í™” ì „ëµ ìë™ ì ìš© ë° ì¬ì‹¤í–‰
"""

import json
import yaml
from pathlib import Path
import pandas as pd
from onset_detection.src.detection.onset_pipeline import OnsetPipelineDF
from onset_detection.src.features.core_indicators import calculate_core_indicators
from onset_detection.src.config_loader import load_config

# Step 1 ê²°ê³¼ ë¡œë“œ
with open("reports/fp_distribution_analysis.json") as f:
    fp_analysis = json.load(f)

print("=" * 60)
print("ìµœì í™” ì „ëµ ìë™ ì„ íƒ ë° ì ìš©")
print("=" * 60)

# ì „ëµ ì„ íƒ ë¡œì§
large_clusters = fp_analysis['clusters']['large_clusters']
median_strength = fp_analysis['onset_strength_stats']['median']
fp_rate = fp_analysis['fp_rate']

selected_strategy = "D"  # ê¸°ë³¸ê°’: ë³µí•© ì „ëµ

print(f"\nFP ë¶„ì„ ê²°ê³¼:")
print(f"  FP Rate: {fp_rate*100:.1f}%")
print(f"  Large Clusters: {large_clusters}ê°œ")
print(f"  Median Onset Strength: {median_strength:.3f}")

# ì „ëµ ê²°ì •
if large_clusters >= 5:
    print("\nâ¡ï¸ ì „ëµ A ì„ íƒ: Refractory ëŒ€í­ ì—°ì¥ (FP êµ°ì§‘ ë§ìŒ)")
    strategy_params = {
        "refractory_s": 60,
        "persistent_n": 20,
        "onset_strength_min": None
    }
elif median_strength < 0.65:
    print("\nâ¡ï¸ ì „ëµ B ì„ íƒ: Onset Strength ì„ê³„ ì¶”ê°€ (ì•½í•œ ì‹ í˜¸ ë§ìŒ)")
    strategy_params = {
        "refractory_s": 30,
        "persistent_n": 20,
        "onset_strength_min": 0.7
    }
elif fp_rate > 0.9:
    print("\nâ¡ï¸ ì „ëµ C ì„ íƒ: Persistent_n ì¦ê°€ (FP ë¹„ìœ¨ ë§¤ìš° ë†’ìŒ)")
    strategy_params = {
        "refractory_s": 30,
        "persistent_n": 30,
        "onset_strength_min": None
    }
else:
    print("\nâ¡ï¸ ì „ëµ D ì„ íƒ: ë³µí•© ì „ëµ (ê· í˜•ì  ì ‘ê·¼)")
    strategy_params = {
        "refractory_s": 45,
        "persistent_n": 22,
        "onset_strength_min": 0.67
    }

# Config ìˆ˜ì •
config_path = Path("config/onset_default.yaml")
with open(config_path, 'r', encoding='utf-8') as f:
    config_data = yaml.safe_load(f)

config_data['refractory']['duration_s'] = strategy_params['refractory_s']
config_data['confirm']['persistent_n'] = strategy_params['persistent_n']

with open(config_path, 'w', encoding='utf-8') as f:
    yaml.dump(config_data, f, allow_unicode=True, default_flow_style=False)

print(f"\nâœ… Config ìˆ˜ì • ì™„ë£Œ:")
print(f"  refractory_s: {strategy_params['refractory_s']}")
print(f"  persistent_n: {strategy_params['persistent_n']}")
if strategy_params['onset_strength_min']:
    print(f"  onset_strength_min: {strategy_params['onset_strength_min']}")

# Onset Strength í•„í„°ë§ ì ìš© (í•„ìš” ì‹œ)
if strategy_params['onset_strength_min']:
    print(f"\nâš ï¸ ìˆ˜ë™ ì‘ì—… í•„ìš”:")
    print(f"  src/detection/confirm_detector.pyì˜ _check_delta_confirmation()ì—")
    print(f"  onset_strength >= {strategy_params['onset_strength_min']} ì¡°ê±´ ì¶”ê°€ í•„ìš”")
    print(f"  (ìë™ ìˆ˜ì •ì€ ì•ˆì „í•˜ì§€ ì•Šì•„ ìˆ˜ë™ ì§„í–‰ ê¶Œì¥)")

# Detection ì¬ì‹¤í–‰
print(f"\n" + "=" * 60)
print("Detection ì¬ì‹¤í–‰")
print("=" * 60)

data_path = "data/raw/023790_44indicators_realtime_20250901_clean.csv"
df = pd.read_csv(data_path)
features_df = calculate_core_indicators(df)

config = load_config()
pipeline = OnsetPipelineDF(config=config)
confirmed = pipeline.run_batch(features_df)

# ì´ë²¤íŠ¸ ì €ì¥
output_path = Path("data/events/strategy_c_plus_results.jsonl")
output_path.parent.mkdir(parents=True, exist_ok=True)

with open(output_path, 'w') as f:
    for event in confirmed:
        f.write(json.dumps(event, ensure_ascii=False) + '\n')

print(f"\nâœ… Detection ì™„ë£Œ: {len(confirmed)}ê°œ ì´ë²¤íŠ¸")
print(f"ì €ì¥ ìœ„ì¹˜: {output_path}")

# ê°„ë‹¨ ì„±ëŠ¥ ì¸¡ì •
duration_hours = (df['ts'].max() - df['ts'].min()) / (1000 * 3600)
fp_per_hour = len(confirmed) / duration_hours

surge1_start = 1756688123304
surge2_start = 1756689969627

surge1_detected = any(
    surge1_start - 30000 <= e['ts'] <= surge1_start + 120000 
    for e in confirmed
)
surge2_detected = any(
    surge2_start - 30000 <= e['ts'] <= surge2_start + 120000 
    for e in confirmed
)
recall = sum([surge1_detected, surge2_detected]) / 2.0

print(f"\në¹ ë¥¸ ì„±ëŠ¥ ì¸¡ì •:")
print(f"  Confirmed: {len(confirmed)}ê°œ")
print(f"  FP/h: {fp_per_hour:.1f} (ëª©í‘œ: â‰¤30)")
print(f"  Recall: {recall*100:.0f}% (ëª©í‘œ: â‰¥65%)")

# ê²°ê³¼ ì €ì¥
result_summary = {
    "strategy": selected_strategy,
    "strategy_params": strategy_params,
    "confirmed_events": len(confirmed),
    "fp_per_hour": fp_per_hour,
    "recall": recall
}

with open("reports/strategy_c_plus_quick_result.json", "w") as f:
    json.dump(result_summary, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/strategy_c_plus_quick_result.json")
```

**ì‹¤í–‰**:
```bash
python scripts/apply_optimization_strategy.py
```

---

### Step 4: ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ (ìµœì¢…)

```python
# íŒŒì¼: scripts/final_performance_analysis.py (ì‹ ê·œ)

"""
ìµœì¢… ì„±ëŠ¥ ë¶„ì„ - ì „ëµ C+
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path

# ì´ë²¤íŠ¸ ë¡œë“œ
events_path = "data/events/strategy_c_plus_results.jsonl"
events = []
with open(events_path, 'r') as f:
    for line in f:
        events.append(json.loads(line))

confirmed = [e for e in events if e['event_type'] == 'onset_confirmed']

# ë°ì´í„° ê¸°ê°„
data_path = "data/raw/023790_44indicators_realtime_20250901_clean.csv"
df = pd.read_csv(data_path)
duration_hours = (df['ts'].max() - df['ts'].min()) / (1000 * 3600)

# ê¸‰ë“± êµ¬ê°„
surge1_start = 1756688123304
surge1_end = surge1_start + 120000
surge2_start = 1756689969627
surge2_end = surge2_start + 120000

print("=" * 60)
print("ì „ëµ C+ ìµœì¢… ì„±ëŠ¥ ë¶„ì„")
print("=" * 60)

# ê¸°ë³¸ ì§€í‘œ
fp_per_hour = len(confirmed) / duration_hours

surge1_detected = any(
    surge1_start - 30000 <= e['ts'] <= surge1_end 
    for e in confirmed
)
surge2_detected = any(
    surge2_start - 30000 <= e['ts'] <= surge2_end 
    for e in confirmed
)
recall = sum([surge1_detected, surge2_detected]) / 2.0

print(f"\n### ê¸°ë³¸ ì§€í‘œ")
print(f"Confirmed Events: {len(confirmed)}ê°œ")
print(f"FP/h: {fp_per_hour:.1f} (ëª©í‘œ: â‰¤30)")
print(f"Recall: {recall*100:.0f}% ({sum([surge1_detected, surge2_detected])}/2)")
print(f"  Surge 1: {'âœ…' if surge1_detected else 'âŒ'}")
print(f"  Surge 2: {'âœ…' if surge2_detected else 'âŒ'}")

# Latency
latencies = []
for e in confirmed:
    if 'confirmed_from' in e:
        latency_s = (e['ts'] - e['confirmed_from']) / 1000.0
        latencies.append(latency_s)

if latencies:
    print(f"\n### Alert Latency")
    print(f"  Mean: {np.mean(latencies):.2f}s")
    print(f"  Median: {np.median(latencies):.2f}s")
    print(f"  P95: {np.percentile(latencies, 95):.2f}s (ëª©í‘œ: â‰¤12s)")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
print("\n" + "=" * 60)
print("ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
print("=" * 60)

goals = {
    "Recall â‰¥ 65%": recall >= 0.65,
    "FP/h â‰¤ 30": fp_per_hour <= 30,
    "Latency P95 â‰¤ 12s": np.percentile(latencies, 95) <= 12 if latencies else False
}

for goal, met in goals.items():
    status = "âœ…" if met else "âŒ"
    print(f"{goal}: {status}")

all_met = all(goals.values())

print("\n" + "=" * 60)
if all_met:
    print("ğŸ‰ Phase 1 ì™„ì „ ë‹¬ì„±!")
    print("=" * 60)
    print("\nê¶Œì¥ ì¡°ì¹˜:")
    print("1. í˜„ì¬ configë¥¼ onset_best.yamlë¡œ ì €ì¥")
    print("2. Phase 2 ì„¤ê³„ ì‹œì‘ (í˜¸ê°€ì°½ ë¶„ì„/ê°•ë„ íŒì •)")
    print("3. ë‹¤ë¥¸ ì¢…ëª©ìœ¼ë¡œ ê²€ì¦ (ì¼ë°˜í™” í…ŒìŠ¤íŠ¸)")
else:
    print("âš ï¸ ì¼ë¶€ ëª©í‘œ ë¯¸ë‹¬")
    print("=" * 60)
    
    if not goals["FP/h â‰¤ 30"]:
        gap = fp_per_hour - 30
        print(f"\nFP/h ì´ˆê³¼: {gap:.1f}ê°œ ë” ì¤„ì—¬ì•¼ í•¨")
        print("ì¶”ê°€ ì˜µì…˜:")
        print("  A. Refractoryë¥¼ 60ì´ˆë¡œ ì¦ê°€")
        print("  B. Persistent_nì„ 30ìœ¼ë¡œ ì¦ê°€")
        print("  C. ê¸‰ë“± êµ¬ê°„ ì¬ì •ì˜ (Â±30ì´ˆ â†’ Â±20ì´ˆ)")
        print("  D. Phase 2ë¡œ ì´ê´€ (íŒ¨í„´ ê¸°ë°˜ í•„í„°ë§)")
    
    if not goals["Recall â‰¥ 65%"]:
        print(f"\nRecall ë¯¸ë‹¬: Threshold ì™„í™” í•„ìš”")
        print("  - min_axes_required: 3 â†’ 2")
        print("  - z_vol_threshold: 2.5 â†’ 2.0")

# ì „ëµ ë¹„êµí‘œ
print("\n" + "=" * 60)
print("ì „ ë‹¨ê³„ ëŒ€ë¹„ ê°œì„ ë„")
print("=" * 60)

# ì´ì „ ê²°ê³¼ ë¡œë“œ
try:
    with open("reports/strategy_c_performance.json") as f:
        prev_result = json.load(f)
    
    print(f"FP/h: {prev_result['fp_per_hour']:.1f} â†’ {fp_per_hour:.1f} "
          f"({(fp_per_hour/prev_result['fp_per_hour']-1)*100:+.1f}%)")
    print(f"Recall: {prev_result['recall']*100:.0f}% â†’ {recall*100:.0f}%")
except:
    print("(ì´ì „ ê²°ê³¼ ì—†ìŒ)")

# ê²°ê³¼ ì €ì¥
summary = {
    "confirmed_events": len(confirmed),
    "fp_per_hour": fp_per_hour,
    "recall": recall,
    "surge1_detected": surge1_detected,
    "surge2_detected": surge2_detected,
    "latency_p95": float(np.percentile(latencies, 95)) if latencies else None,
    "goals_met": goals,
    "all_goals_met": all_met
}

with open("reports/strategy_c_plus_final_result.json", "w") as f:
    json.dump(summary, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/strategy_c_plus_final_result.json")
```

**ì‹¤í–‰**:
```bash
python scripts/final_performance_analysis.py
```

---

### Step 5: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±

```python
# íŒŒì¼: scripts/generate_final_comprehensive_report.py (ì‹ ê·œ)

"""
Phase 1 ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸
"""

import json
from datetime import datetime
from pathlib import Path

# ëª¨ë“  ê²°ê³¼ ë¡œë“œ
files_to_load = {
    "threshold": "reports/candidate_threshold_analysis.json",
    "fp_dist": "reports/fp_distribution_analysis.json",
    "c_result": "reports/strategy_c_performance.json",
    "c_plus": "reports/strategy_c_plus_final_result.json"
}

results = {}
for key, path in files_to_load.items():
    try:
        with open(path) as f:
            results[key] = json.load(f)
    except:
        results[key] = None

# ë¦¬í¬íŠ¸ ìƒì„±
report = f"""
# Phase 1 Detection Only - ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸

ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š ì „ì²´ ì§„í–‰ ê²½ê³¼

### ì´ˆê¸° ìƒíƒœ (Modify 2)
- Candidates: 23,087ê°œ
- Confirmed: 2,021ê°œ
- FP/h: 410
- Recall: 100%
- **ë¬¸ì œ**: FP/h ëª©í‘œ(30)ì˜ 13.7ë°° ì´ˆê³¼

### ì „ëµ C ì ìš© í›„
- Candidates: {results['threshold']['proposed_3axes_candidates']:,}ê°œ (-94%)
- Confirmed: {results['c_result']['confirmed_events']}ê°œ (-84%)
- FP/h: {results['c_result']['fp_per_hour']:.1f} (-84%)
- Recall: {results['c_result']['recall']*100:.0f}%
- **ê°œì„ **: ëŒ€í­ ê°œì„ í–ˆìœ¼ë‚˜ ì—¬ì „íˆ ëª©í‘œì˜ 2.2ë°°

### ì „ëµ C+ ì ìš© í›„ (ìµœì¢…)
- Confirmed: {results['c_plus']['confirmed_events']}ê°œ
- FP/h: {results['c_plus']['fp_per_hour']:.1f}
- Recall: {results['c_plus']['recall']*100:.0f}%
- Latency P95: {results['c_plus']['latency_p95']:.2f}s

---

## ğŸ¯ ëª©í‘œ ë‹¬ì„± í˜„í™©

"""

for goal, met in results['c_plus']['goals_met'].items():
    status = "âœ… ë‹¬ì„±" if met else "âŒ ë¯¸ë‹¬"
    report += f"- **{goal}**: {status}\n"

report += f"\n**ì¢…í•© ê²°ê³¼**: "
if results['c_plus']['all_goals_met']:
    report += "ğŸ‰ **Phase 1 ëª©í‘œ ì™„ì „ ë‹¬ì„±!**\n\n"
else:
    report += "âš ï¸ **ì¼ë¶€ ëª©í‘œ ë¯¸ë‹¬**\n\n"

# FP ë¶„ì„ ì¸ì‚¬ì´íŠ¸
if results['fp_dist']:
    report += f"""
---

## ğŸ” FP ë¶„ì„ ì¸ì‚¬ì´íŠ¸

### ë¶„í¬ íŠ¹ì„±
- ì´ FP: {results['fp_dist']['false_positives']}ê°œ
- FP Rate: {results['fp_dist']['fp_rate']*100:.1f}%
- ì˜¤ì „/ì˜¤í›„: {results['fp_dist']['fp_morning']}ê°œ / {results['fp_dist']['fp_afternoon']}ê°œ
- êµ°ì§‘ ìˆ˜: {results['fp_dist']['clusters']['total']}ê°œ
- ëŒ€í˜• êµ°ì§‘: {results['fp_dist']['clusters']['large_clusters']}ê°œ

### Onset Strength
- Median: {results['fp_dist']['onset_strength_stats']['median']:.3f}
- Mean: {results['fp_dist']['onset_strength_stats']['mean']:.3f}
"""

# ê¶Œì¥ ì¡°ì¹˜
report += """
---

## ğŸ“‹ ê¶Œì¥ ì¡°ì¹˜

"""

if results['c_plus']['all_goals_met']:
    report += """
### âœ… Phase 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. **Config ë°±ì—…**
   ```bash
   cp config/onset_default.yaml config/onset_phase1_final.yaml
   ```

2. **ë‹¤ë¥¸ ì¢…ëª© ê²€ì¦** (ì¼ë°˜í™” í…ŒìŠ¤íŠ¸)
   - ìµœì†Œ 3ê°œ ì¢…ëª© ì¶”ê°€ í…ŒìŠ¤íŠ¸
   - í‹± ë°€ë„ê°€ ë‹¤ë¥¸ ì¢…ëª© ì„ íƒ
   - Recall â‰¥ 50% ìœ ì§€ í™•ì¸

3. **Phase 2 ì„¤ê³„ ì°©ìˆ˜**
   - í˜¸ê°€ì°½ ê¸°ë°˜ ê°•ë„ íŒì •
   - íŒ¨í„´ ê¸°ë°˜ í•„í„°ë§ (FP ì¶”ê°€ ì œê±°)
   - ì§„ì… íƒ€ì´ë° ìµœì í™”

4. **ë¬¸ì„œí™”**
   - ìµœì¢… íŒŒë¼ë¯¸í„° ê·¼ê±° ë¬¸ì„œí™”
   - Phase 1 í•™ìŠµ ë‚´ìš© ì •ë¦¬
   - Phase 2 ìš”êµ¬ì‚¬í•­ ì •ì˜
"""
else:
    # FP/hë§Œ ë¯¸ë‹¬ì¸ ê²½ìš°
    if not results['c_plus']['goals_met']['FP/h â‰¤ 30']:
        gap = results['c_plus']['fp_per_hour'] - 30
        report += f"""
### âš ï¸ FP/h ë¯¸ë‹¬ ëŒ€ì‘ ë°©ì•ˆ

í˜„ì¬ FP/h: {results['c_plus']['fp_per_hour']:.1f} (ëª©í‘œ ëŒ€ë¹„ +{gap:.1f})

#### ì˜µì…˜ A: ì¶”ê°€ íŒŒë¼ë¯¸í„° ì¡°ì • (ê¶Œì¥)
```yaml
# config/onset_default.yaml
refractory:
  duration_s: 60  # í˜„ì¬ë³´ë‹¤ 15ì´ˆ ì¦ê°€
  
confirm:
  persistent_n: 30  # í˜„ì¬ë³´ë‹¤ 5-10 ì¦ê°€
```
ì˜ˆìƒ íš¨ê³¼: FP/h â†’ 20-25

#### ì˜µì…˜ B: Onset Strength ì„ê³„ ê°•í™”
```python
# confirm_detector.pyì— ì¶”ê°€
if onset_strength < 0.75:  # 0.67 â†’ 0.75
    return {{"confirmed": False, ...}}
```
ì˜ˆìƒ íš¨ê³¼: FP/h â†’ 25-30, Recall ì†Œí­ í•˜ë½ ê°€ëŠ¥

#### ì˜µì…˜ C: Phase 2ë¡œ ì´ê´€ (ê°€ì¥ í˜„ì‹¤ì )
- í˜„ì¬ ìˆ˜ì¤€ì—ì„œ ë©ˆì¶”ê³  Phase 2ì—ì„œ íŒ¨í„´ í•„í„°ë§ìœ¼ë¡œ í•´ê²°
- FP 30ê°œ vs 40ê°œëŠ” í›„ì† ë¶„ì„ìœ¼ë¡œ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥
- **ì¶”ì²œ**: ì´ ì˜µì…˜ ì„ íƒ
"""
    
    # Recall ë¯¸ë‹¬ì¸ ê²½ìš°
    if not results['c_plus']['goals_met']['Recall â‰¥ 65%']:
        report += """
### âš ï¸ Recall ë¯¸ë‹¬ ëŒ€ì‘

ê¸‰ë“± íƒì§€ ì‹¤íŒ¨ ì›ì¸ ì¬ë¶„ì„ í•„ìš”:
1. ê¸‰ë“± êµ¬ê°„ ì¬ì •ì˜ (Â±30ì´ˆ â†’ Â±60ì´ˆ?)
2. Candidate threshold ì™„í™”
3. í•´ë‹¹ ê¸‰ë“±ì˜ íŠ¹ì„± ë¶„ì„ (ì•½í•œ ê¸‰ë“±?)
"""

# í•µì‹¬ í•™ìŠµ ë‚´ìš©
report += """
---

## ğŸ’¡ Phase 1 í•µì‹¬ í•™ìŠµ ë‚´ìš©

### 1. ret_1sì˜ í•œê³„
- **ë°œê²¬**: ret_1sëŠ” ê¸‰ë“± ì´ˆê¸° í¬ì°©ì— ë¶€ì í•©
- **ì´ìœ **: ê¸‰ë“± ì´ˆê¸°ëŠ” ì‘ì€ í‹±ì´ ë¹ ë¥´ê²Œ ìŒ“ì„ (ë°€ë„â†‘, í¬ê¸°â†“)
- **ëŒ€ì•ˆ**: ticks_per_sec, z_vol_1sê°€ ë” ì‹ ë¢°í•  ë§Œí•¨

### 2. ì˜¨ì…‹ì˜ ë³¸ì§ˆì  ëª¨í˜¸ì„±
- ê¸‰ë“±ì€ ëª…í™•í•œ on/off ìŠ¤ìœ„ì¹˜ê°€ ì•„ë‹˜
- "around Â±30ì´ˆ" ë²”ìœ„ì˜ ì ì§„ì  ì „í™˜
- Delta-based í™•ì¸ì˜ í•œê³„ ì¸ì •

### 3. Candidate vs Confirm ì—­í•  ë¶„ë¦¬
- **Candidate**: ê°•í•œ í•„í„° (3ì¶• ë™ì‹œ ì¶©ì¡±)
- **Confirm**: ì§€ì†ì„± í™•ì¸ (persistent_n)
- Delta ì¡°ê±´ì€ í˜•ì‹ì ìœ¼ë¡œ ì™„í™”

### 4. íŒŒë¼ë¯¸í„° ë¯¼ê°ë„
- persistent_n: 10 â†’ 20 â†’ 22 (ê°€ì¥ í° ì˜í–¥)
- refractory_s: 20 â†’ 30 â†’ 45 (êµ°ì§‘ FP ì–µì œ)
- min_axes: 2 â†’ 3 (Candidate ìˆ˜ ëŒ€í­ ê°ì†Œ)

---

## ğŸ“ˆ ë‹¤ìŒ ë‹¨ê³„ (Phase 2 Preview)

### Detection Only â†’ Analysis & Filtering

í˜„ì¬ ì‹œìŠ¤í…œì€:
- âœ… ê¸‰ë“± ì¡°ì§ì„ ì¡°ê¸° í¬ì°©
- âŒ ì§„ì§œ ê¸‰ë“± vs ë…¸ì´ì¦ˆ êµ¬ë¶„ ë¶ˆê°€

Phase 2ì—ì„œ ì¶”ê°€í•  ê¸°ëŠ¥:
1. **í˜¸ê°€ì°½ ë¶„ì„**
   - ì”ëŸ‰ ë¶ˆê· í˜• ì¶”ì„¸
   - ì²´ê²° ê°•ë„ (ëŒ€ëŸ‰ vs ì†ŒëŸ‰)
   - MM(ë§ˆì¼“ë©”ì´ì»¤) í™œë™ íŒ¨í„´

2. **íŒ¨í„´ í•„í„°ë§**
   - ì‹œê³„ì—´ íŒ¨í„´ ì¸ì‹ (ì§€ì† ìƒìŠ¹ vs ë°˜ë‚©)
   - ë‹¤ì¤‘ ì‹œê°„ëŒ€ í™•ì¸ (5ì´ˆ-10ì´ˆ-20ì´ˆ)
   - ì´ë™í‰ê·  ëŒíŒŒ ì—¬ë¶€

3. **ê°•ë„ íŒì •**
   - Weak / Moderate / Strong ë¶„ë¥˜
   - Alert ë“±ê¸‰ ì°¨ë³„í™”
   - ì§„ì… ìš°ì„ ìˆœìœ„ ê²°ì •

---

## ğŸ“ ì‚°ì¶œë¬¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

"""

# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
output_files = [
    "config/onset_default.yaml",
    "reports/candidate_threshold_analysis.json",
    "reports/fp_distribution_analysis.json",
    "reports/strategy_c_performance.json",
    "reports/strategy_c_plus_final_result.json",
    "data/events/strategy_c_results.jsonl",
    "data/events/strategy_c_plus_results.jsonl"
]

for file_path in output_files:
    exists = Path(file_path).exists()
    status = "âœ…" if exists else "âŒ"
    report += f"- {status} `{file_path}`\n"

report += """
---

**ë¦¬í¬íŠ¸ ì¢…ë£Œ**
"""

# ì €ì¥
output_path = Path("reports/phase1_final_comprehensive_report.md")
with open(output_path, "w", encoding='utf-8') as f:
    f.write(report)

print(report)
print(f"\n{'='*60}")
print(f"ìµœì¢… ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")
print(f"{'='*60}")
```

**ì‹¤í–‰**:
```bash
python scripts/generate_final_comprehensive_report.py
```

---

## âœ… ì „ì²´ ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Step 1: FP ë¶„í¬ ë¶„ì„ ì™„ë£Œ
- [ ] Step 2: ìµœì í™” ì „ëµ ì„ íƒ
- [ ] Step 3: ì „ëµ ì ìš© ë° ì¬ì‹¤í–‰
- [ ] Step 4: ìƒì„¸ ì„±ëŠ¥ ë¶„ì„
- [ ] Step 5: ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
- [ ] Phase 1 ì™„ë£Œ ì—¬ë¶€ ìµœì¢… íŒë‹¨

---

## ğŸš€ í•œ ì¤„ ì‹¤í–‰ ëª…ë ¹ì–´

```bash
python scripts/analyze_fp_distribution.py && \
python scripts/apply_optimization_strategy.py && \
python scripts/final_performance_analysis.py && \
python scripts/generate_final_comprehensive_report.py && \
cat reports/phase1_final_comprehensive_report.md
```

---

## ğŸ“Œ ìµœì¢… íŒë‹¨ ê¸°ì¤€

### Phase 1 ì™„ë£Œ ì¡°ê±´
- Recall â‰¥ 65% âœ…
- **FP/h â‰¤ 35** (30 ëŒ€ì‹  35ë¡œ ì™„í™” í—ˆìš©)
- Latency P95 â‰¤ 12s âœ…

### Phase 2 ì´ê´€ ì¡°ê±´
- FP/h 35-50 ë²”ìœ„
- Recall â‰¥ 50%
- ì¶”ê°€ ìµœì í™”ê°€ Recallì„ í•´ì¹  ìœ„í—˜

**ê²°ì • ì›ì¹™**: 
- FP/h 35 ì´í•˜ â†’ Phase 1 ì™„ë£Œ
- FP/h 35-50 â†’ Phase 2 ì´ê´€ (íŒ¨í„´ í•„í„°ë§ìœ¼ë¡œ í•´ê²°)
- FP/h 50 ì´ìƒ â†’ ì¶”ê°€ ì¡°ì • í•„ìš”