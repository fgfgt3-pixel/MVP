# Phase 1 ì¶”ê°€ ê²€ì¦ ì‘ì—… ì§€ì‹œì„œ (Claude Code ì‹¤í–‰ìš©)

## ğŸ¯ ì‘ì—… ëª©í‘œ
1. **íƒ€ì´ë° ê²€ì¦**: ê¸‰ë“± í¬ì°©ì´ ì–¼ë§ˆë‚˜ ë¹ ë¥¸ì§€ ì‹œê°ì  í™•ì¸
2. **ì‹œê°í™”**: ê¸‰ë“± êµ¬ê°„ + íƒì§€ ì‹œì ì„ ì°¨íŠ¸ë¡œ í‘œí˜„
3. **ì„ê³„ ì™„í™”**: onset_strengthë¥¼ 0.67-0.68ë¡œ ë‚®ì¶° ì•½í•œ ê¸‰ë“± ì¤‘ ê°•í•œ ê²ƒ í¬ì°©

---

## ğŸ“‹ ì‘ì—… ìˆœì„œ (ì—°ì† ì‹¤í–‰)

### Step 1: ê¸‰ë“± íƒ€ì´ë° ìƒì„¸ ë¶„ì„

```python
# íŒŒì¼: scripts/analyze_detection_timing.py (ì‹ ê·œ)

"""
ê¸‰ë“± íƒì§€ íƒ€ì´ë° ë¶„ì„
ëª©ì : ê° ê¸‰ë“±ì˜ ì‹œì‘/í”¼í¬ ëŒ€ë¹„ ì–¸ì œ íƒì§€í–ˆëŠ”ì§€ í™•ì¸
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime
from pathlib import Path

# ê¸‰ë“± ì •ì˜ (ì‹œì‘, í”¼í¬, ì¢…ë£Œ)
surges_023790 = [
    {"name": "Surge1", "start": 1756688123304, "peak_offset": 120000, "strength": "ì¤‘ê°„"},
    {"name": "Surge2", "start": 1756689969627, "peak_offset": 120000, "strength": "ì¤‘ê°„"}
]

surges_413630 = [
    {"name": "Surge1", "time": "09:09-09:15", "start_offset": 0, "peak_offset": 240000, "strength": "ì¤‘ê°„"},
    {"name": "Surge2", "time": "10:01-10:14", "start_offset": 0, "peak_offset": 660000, "strength": "ê°•í•œ"},
    {"name": "Surge3", "time": "11:46-11:50", "start_offset": 0, "peak_offset": 180000, "strength": "ì•½í•œ"},
    {"name": "Surge4", "time": "13:29-13:37", "start_offset": 0, "peak_offset": 300000, "strength": "ì¤‘ê°„"},
    {"name": "Surge5", "time": "14:09-14:12", "start_offset": 0, "peak_offset": 120000, "strength": "ì•½í•œ"}
]

def load_events(file_path):
    """ì´ë²¤íŠ¸ ë¡œë“œ"""
    events = []
    with open(file_path, 'r') as f:
        for line in f:
            events.append(json.loads(line))
    return [e for e in events if e['event_type'] == 'onset_confirmed']

def ms_to_kst(ms):
    """ë°€ë¦¬ì´ˆë¥¼ KST datetimeìœ¼ë¡œ ë³€í™˜"""
    return pd.to_datetime(ms, unit='ms', utc=True).tz_convert('Asia/Seoul')

def analyze_file_timing(events, surges, data_path, file_name):
    """íŒŒì¼ë³„ íƒ€ì´ë° ë¶„ì„"""
    df = pd.read_csv(data_path)
    
    print(f"\n{'='*60}")
    print(f"{file_name} íƒ€ì´ë° ë¶„ì„")
    print(f"{'='*60}")
    
    results = []
    
    for surge in surges:
        surge_name = surge['name']
        surge_strength = surge['strength']
        
        # ê¸‰ë“± ì‹œì‘ ì‹œì 
        if 'start' in surge:
            surge_start = surge['start']
        else:
            # 413630ì˜ ê²½ìš° ë°ì´í„°ì—ì„œ ì‹œì‘ ì‹œì  ì¶”ì¶œ
            time_str = surge['time'].split('-')[0]  # "09:09"
            hour, minute = map(int, time_str.split(':'))
            
            # ë°ì´í„°ì˜ ì²« timestamp ê°€ì ¸ì˜¤ê¸°
            first_ts = df['ts'].min()
            first_dt = ms_to_kst(first_ts)
            
            # í•´ë‹¹ ì‹œê°ìœ¼ë¡œ ì„¤ì •
            surge_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
            surge_start = int(surge_dt.timestamp() * 1000)
        
        surge_peak = surge_start + surge.get('peak_offset', 120000)
        
        # í•´ë‹¹ êµ¬ê°„ íƒì§€ ì´ë²¤íŠ¸ ì°¾ê¸°
        detected_events = [
            e for e in events 
            if surge_start - 30000 <= e['ts'] <= surge_peak + 30000
        ]
        
        if not detected_events:
            print(f"\nâŒ {surge_name} ({surge_strength}): íƒì§€ ì‹¤íŒ¨")
            results.append({
                "surge": surge_name,
                "strength": surge_strength,
                "detected": False,
                "detection_time_kst": None,
                "latency_from_start": None,
                "latency_to_peak": None
            })
            continue
        
        # ê°€ì¥ ë¹ ë¥¸ íƒì§€ ì‹œì 
        first_detection = min(detected_events, key=lambda x: x['ts'])
        detection_ts = first_detection['ts']
        
        # íƒ€ì´ë° ê³„ì‚°
        latency_from_start = (detection_ts - surge_start) / 1000.0  # ì´ˆ
        latency_to_peak = (surge_peak - detection_ts) / 1000.0
        
        # ë°±ë¶„ìœ¨ ê³„ì‚° (ì‹œì‘â†’í”¼í¬ êµ¬ê°„ ì¤‘ ëª‡ %ì—ì„œ íƒì§€)
        surge_duration = (surge_peak - surge_start) / 1000.0
        detection_position_pct = (latency_from_start / surge_duration) * 100
        
        detection_dt = ms_to_kst(detection_ts)
        
        print(f"\nâœ… {surge_name} ({surge_strength}):")
        print(f"   íƒì§€ ì‹œê°: {detection_dt.strftime('%H:%M:%S.%f')[:-3]}")
        print(f"   ê¸‰ë“± ì‹œì‘ í›„: {latency_from_start:+.1f}ì´ˆ")
        print(f"   í”¼í¬ê¹Œì§€ ë‚¨ì€ ì‹œê°„: {latency_to_peak:.1f}ì´ˆ")
        print(f"   íƒì§€ ìœ„ì¹˜: ì‹œì‘â†’í”¼í¬ êµ¬ê°„ì˜ {detection_position_pct:.1f}%")
        
        # í‰ê°€
        if latency_from_start < 5:
            quality = "â­â­â­ ë§¤ìš° ë¹ ë¦„"
        elif latency_from_start < 15:
            quality = "â­â­ ë¹ ë¦„"
        elif latency_from_start < 30:
            quality = "â­ ë³´í†µ"
        else:
            quality = "âš ï¸ ëŠë¦¼"
        
        print(f"   í‰ê°€: {quality}")
        
        results.append({
            "surge": surge_name,
            "strength": surge_strength,
            "detected": True,
            "detection_time_kst": detection_dt.strftime('%H:%M:%S.%f')[:-3],
            "latency_from_start": latency_from_start,
            "latency_to_peak": latency_to_peak,
            "detection_position_pct": detection_position_pct,
            "quality": quality
        })
    
    return results

# 023790 ë¶„ì„
events_023790 = load_events("data/events/strategy_c_plus_023790.jsonl")
results_023790 = analyze_file_timing(
    events_023790, 
    surges_023790, 
    "data/raw/023790_44indicators_realtime_20250901_clean.csv",
    "023790"
)

# 413630 ë¶„ì„
events_413630 = load_events("data/events/strategy_c_plus_413630.jsonl")
results_413630 = analyze_file_timing(
    events_413630,
    surges_413630,
    "data/raw/413630_44indicators_realtime_20250901_clean.csv",
    "413630"
)

# ì¢…í•© í†µê³„
print(f"\n{'='*60}")
print("ì¢…í•© íƒ€ì´ë° í†µê³„")
print(f"{'='*60}")

all_results = results_023790 + results_413630
detected_results = [r for r in all_results if r['detected']]

if detected_results:
    latencies = [r['latency_from_start'] for r in detected_results]
    positions = [r['detection_position_pct'] for r in detected_results]
    
    print(f"\níƒì§€ëœ ê¸‰ë“±: {len(detected_results)}ê°œ")
    print(f"ê¸‰ë“± ì‹œì‘ í›„ í‰ê·  íƒì§€: {np.mean(latencies):.1f}ì´ˆ")
    print(f"ê¸‰ë“± ì‹œì‘ í›„ ì¤‘ì•™ê°’: {np.median(latencies):.1f}ì´ˆ")
    print(f"í‰ê·  íƒì§€ ìœ„ì¹˜: ì‹œì‘â†’í”¼í¬ì˜ {np.mean(positions):.1f}%")

# ê°•ë„ë³„ ë¶„ì„
print(f"\n{'='*60}")
print("ê°•ë„ë³„ íƒ€ì´ë° ë¶„ì„")
print(f"{'='*60}")

strengths = ["ê°•í•œ", "ì¤‘ê°„", "ì•½í•œ"]
for strength in strengths:
    strength_results = [r for r in detected_results if r['strength'] == strength]
    if strength_results:
        avg_latency = np.mean([r['latency_from_start'] for r in strength_results])
        avg_position = np.mean([r['detection_position_pct'] for r in strength_results])
        print(f"\n{strength} ê¸‰ë“± ({len(strength_results)}ê°œ):")
        print(f"  í‰ê·  íƒì§€: ì‹œì‘ í›„ {avg_latency:.1f}ì´ˆ")
        print(f"  í‰ê·  ìœ„ì¹˜: {avg_position:.1f}%")

# ê²°ê³¼ ì €ì¥
summary = {
    "023790": results_023790,
    "413630": results_413630,
    "summary": {
        "total_surges": len(all_results),
        "detected_surges": len(detected_results),
        "avg_latency_from_start": float(np.mean(latencies)) if latencies else None,
        "median_latency_from_start": float(np.median(latencies)) if latencies else None,
        "avg_detection_position_pct": float(np.mean(positions)) if positions else None
    }
}

Path("reports").mkdir(exist_ok=True)
with open("reports/detection_timing_analysis.json", "w") as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"\nê²°ê³¼ ì €ì¥: reports/detection_timing_analysis.json")
```

**ì‹¤í–‰**:
```bash
python scripts/analyze_detection_timing.py
```

---

### Step 2: Jupyter ì‹œê°í™” ë…¸íŠ¸ë¶ ìƒì„±

```python
# íŒŒì¼: notebooks/visualize_detection_timing.ipynb (ì‹ ê·œ)

# Cell 1: Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import json
from datetime import datetime
from pathlib import Path

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (16, 10)
plt.rcParams['font.size'] = 10

# Cell 2: Load Data Functions
def load_csv_data(file_path):
    """CSV ë°ì´í„° ë¡œë“œ ë° datetime ë³€í™˜"""
    df = pd.read_csv(file_path)
    df['datetime'] = pd.to_datetime(df['ts'], unit='ms', utc=True).dt.tz_convert('Asia/Seoul')
    return df

def load_events(file_path):
    """ì´ë²¤íŠ¸ ë¡œë“œ"""
    events = []
    with open(file_path, 'r') as f:
        for line in f:
            events.append(json.loads(line))
    
    confirmed = [e for e in events if e['event_type'] == 'onset_confirmed']
    
    # datetime ì¶”ê°€
    for e in confirmed:
        e['datetime'] = pd.to_datetime(e['ts'], unit='ms', utc=True).tz_convert('Asia/Seoul')
    
    return confirmed

# Cell 3: Plot Function
def plot_surge_detection(df, events, surge_info, title):
    """ê¸‰ë“± êµ¬ê°„ê³¼ íƒì§€ ì‹œì  ì‹œê°í™”"""
    fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)
    
    # 1. Price chart
    ax1 = axes[0]
    ax1.plot(df['datetime'], df['price'], linewidth=0.5, color='black', alpha=0.7, label='Price')
    ax1.set_ylabel('Price', fontsize=12, fontweight='bold')
    ax1.legend(loc='upper left')
    ax1.grid(True, alpha=0.3)
    
    # 2. Volume
    ax2 = axes[1]
    ax2.bar(df['datetime'], df['volume'], width=0.0001, color='blue', alpha=0.5, label='Volume')
    ax2.set_ylabel('Volume', fontsize=12, fontweight='bold')
    ax2.legend(loc='upper left')
    ax2.grid(True, alpha=0.3)
    
    # 3. Ticks per second
    ax3 = axes[2]
    if 'ticks_per_sec' in df.columns:
        ax3.plot(df['datetime'], df['ticks_per_sec'], linewidth=1, color='green', alpha=0.7, label='Ticks/sec')
    ax3.set_ylabel('Ticks per Second', fontsize=12, fontweight='bold')
    ax3.set_xlabel('Time (KST)', fontsize=12, fontweight='bold')
    ax3.legend(loc='upper left')
    ax3.grid(True, alpha=0.3)
    
    # ê¸‰ë“± êµ¬ê°„ í‘œì‹œ
    for surge in surge_info:
        surge_start = pd.to_datetime(surge['start'], unit='ms', utc=True).tz_convert('Asia/Seoul')
        surge_peak = surge_start + pd.Timedelta(milliseconds=surge.get('peak_offset', 120000))
        
        color_map = {"ê°•í•œ": "red", "ì¤‘ê°„": "orange", "ì•½í•œ": "yellow"}
        color = color_map.get(surge['strength'], 'gray')
        
        for ax in axes:
            ax.axvspan(surge_start, surge_peak, alpha=0.15, color=color, 
                      label=f"{surge['name']} ({surge['strength']})")
    
    # íƒì§€ ì‹œì  í‘œì‹œ
    for event in events:
        event_dt = event['datetime']
        
        for ax in axes:
            ax.axvline(event_dt, color='purple', linestyle='--', linewidth=2, alpha=0.7)
            ax.text(event_dt, ax.get_ylim()[1]*0.95, 'ğŸš¨', 
                   fontsize=16, ha='center', va='top')
    
    # ë²”ë¡€ ì •ë¦¬
    handles, labels = ax1.get_legend_handles_labels()
    ax1.legend(handles[:5], labels[:5], loc='upper left', fontsize=9)
    
    # ì‹œê°„ í¬ë§·
    ax3.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')
    
    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)
    plt.tight_layout()
    
    return fig

# Cell 4: 023790 ì‹œê°í™”
df_023790 = load_csv_data("data/raw/023790_44indicators_realtime_20250901_clean.csv")
events_023790 = load_events("data/events/strategy_c_plus_023790.jsonl")

surges_023790 = [
    {"name": "Surge1", "start": 1756688123304, "peak_offset": 120000, "strength": "ì¤‘ê°„"},
    {"name": "Surge2", "start": 1756689969627, "peak_offset": 120000, "strength": "ì¤‘ê°„"}
]

fig1 = plot_surge_detection(df_023790, events_023790, surges_023790, 
                            "023790 ê¸‰ë“± íƒì§€ íƒ€ì´ë° (ì „ëµ C+)")
plt.savefig("reports/plots/023790_detection_timing.png", dpi=150, bbox_inches='tight')
plt.show()

# Cell 5: 413630 ì‹œê°í™”
df_413630 = load_csv_data("data/raw/413630_44indicators_realtime_20250901_clean.csv")
events_413630 = load_events("data/events/strategy_c_plus_413630.jsonl")

# 413630 surge ì‹œì‘ ì‹œì  ê³„ì‚°
first_ts = df_413630['ts'].min()
first_dt = pd.to_datetime(first_ts, unit='ms', utc=True).tz_convert('Asia/Seoul')

surge_times = [
    ("09:09", 240000, "ì¤‘ê°„"),
    ("10:01", 660000, "ê°•í•œ"),
    ("11:46", 180000, "ì•½í•œ"),
    ("13:29", 300000, "ì¤‘ê°„"),
    ("14:09", 120000, "ì•½í•œ")
]

surges_413630 = []
for i, (time_str, peak_offset, strength) in enumerate(surge_times, 1):
    hour, minute = map(int, time_str.split(':'))
    surge_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
    surge_start_ms = int(surge_dt.timestamp() * 1000)
    
    surges_413630.append({
        "name": f"Surge{i}",
        "start": surge_start_ms,
        "peak_offset": peak_offset,
        "strength": strength
    })

fig2 = plot_surge_detection(df_413630, events_413630, surges_413630,
                            "413630 ê¸‰ë“± íƒì§€ íƒ€ì´ë° (ì „ëµ C+)")
plt.savefig("reports/plots/413630_detection_timing.png", dpi=150, bbox_inches='tight')
plt.show()

# Cell 6: Summary Statistics
print("="*60)
print("íƒì§€ íƒ€ì´ë° ìš”ì•½")
print("="*60)

with open("reports/detection_timing_analysis.json") as f:
    timing_data = json.load(f)

summary = timing_data['summary']
print(f"\nì „ì²´ ê¸‰ë“±: {summary['total_surges']}ê°œ")
print(f"íƒì§€ëœ ê¸‰ë“±: {summary['detected_surges']}ê°œ")
print(f"í‰ê·  íƒì§€ ì§€ì—°: {summary['avg_latency_from_start']:.1f}ì´ˆ")
print(f"í‰ê·  íƒì§€ ìœ„ì¹˜: ì‹œì‘â†’í”¼í¬ì˜ {summary['avg_detection_position_pct']:.1f}%")

print("\n"+"="*60)
```

**ì‹¤í–‰ ë°©ë²•**:
```bash
# Jupyter ì„¤ì¹˜ (ì—†ìœ¼ë©´)
pip install jupyter matplotlib

# ë…¸íŠ¸ë¶ ì‹¤í–‰
jupyter notebook notebooks/visualize_detection_timing.ipynb
```

---

### Step 3: onset_strength ì™„í™” ë° ì¬ì‹¤í–‰

```python
# íŒŒì¼: scripts/relax_onset_strength.py (ì‹ ê·œ)

"""
onset_strength ì„ê³„ ì™„í™”
ëª©ì : 0.70 â†’ 0.67ë¡œ ë‚®ì¶° ì•½í•œ ê¸‰ë“± ì¤‘ ê°•í•œ ê²ƒ í¬ì°©
"""

import re
from pathlib import Path

print("="*60)
print("onset_strength ì„ê³„ ì™„í™”")
print("="*60)

# confirm_detector.py ìˆ˜ì •
detector_path = Path("onset_detection/src/detection/confirm_detector.py")

with open(detector_path, 'r', encoding='utf-8') as f:
    content = f.read()

# í˜„ì¬ ì„ê³„ê°’ ì°¾ê¸°
current_threshold = None
match = re.search(r'if onset_strength < (0\.\d+):', content)
if match:
    current_threshold = float(match.group(1))
    print(f"\ní˜„ì¬ onset_strength ì„ê³„: {current_threshold}")
else:
    print("\nâš ï¸ onset_strength í•„í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("confirm_detector.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)

# ìƒˆ ì„ê³„ê°’
new_threshold = 0.67
print(f"ìƒˆ onset_strength ì„ê³„: {new_threshold}")

# êµì²´
new_content = content.replace(
    f'if onset_strength < {current_threshold}:',
    f'if onset_strength < {new_threshold}:'
)

# ì €ì¥
with open(detector_path, 'w', encoding='utf-8') as f:
    f.write(new_content)

print(f"âœ… {detector_path} ìˆ˜ì • ì™„ë£Œ")

# Detection ì¬ì‹¤í–‰
print("\n"+"="*60)
print("Detection ì¬ì‹¤í–‰")
print("="*60)

import pandas as pd
from onset_detection.src.detection.onset_pipeline import OnsetPipelineDF
from onset_detection.src.features.core_indicators import calculate_core_indicators
from onset_detection.src.config_loader import load_config
import json

files = [
    ("023790", "data/raw/023790_44indicators_realtime_20250901_clean.csv"),
    ("413630", "data/raw/413630_44indicators_realtime_20250901_clean.csv")
]

results = {}

for stock_code, data_path in files:
    print(f"\n[{stock_code}] ì²˜ë¦¬ ì¤‘...")
    
    df = pd.read_csv(data_path)
    features_df = calculate_core_indicators(df)
    
    config = load_config()
    pipeline = OnsetPipelineDF(config=config)
    confirmed = pipeline.run_batch(features_df)
    
    # ì €ì¥
    output_path = Path(f"data/events/strategy_c_plus_relaxed_{stock_code}.jsonl")
    with open(output_path, 'w') as f:
        for event in confirmed:
            f.write(json.dumps(event, ensure_ascii=False) + '\n')
    
    duration_hours = (df['ts'].max() - df['ts'].min()) / (1000 * 3600)
    fp_per_hour = len(confirmed) / duration_hours
    
    print(f"  Confirmed: {len(confirmed)}ê°œ")
    print(f"  FP/h: {fp_per_hour:.1f}")
    
    results[stock_code] = {
        "confirmed": len(confirmed),
        "fp_per_hour": fp_per_hour
    }

# Recall ì¬ê³„ì‚°
print("\n"+"="*60)
print("Recall ì¬ê³„ì‚°")
print("="*60)

# 023790
events_023790 = []
with open("data/events/strategy_c_plus_relaxed_023790.jsonl") as f:
    for line in f:
        events_023790.append(json.loads(line))

surge1_023790 = 1756688123304
surge2_023790 = 1756689969627

s1_detected = any(surge1_023790 - 30000 <= e['ts'] <= surge1_023790 + 120000 for e in events_023790)
s2_detected = any(surge2_023790 - 30000 <= e['ts'] <= surge2_023790 + 120000 for e in events_023790)

recall_023790 = sum([s1_detected, s2_detected]) / 2.0
print(f"\n023790 Recall: {recall_023790*100:.0f}% ({sum([s1_detected, s2_detected])}/2)")

# 413630
events_413630 = []
with open("data/events/strategy_c_plus_relaxed_413630.jsonl") as f:
    for line in f:
        events_413630.append(json.loads(line))

# 413630 surge ì‹œì‘ ê³„ì‚° (ê°„ë‹¨ ë²„ì „)
df_413630 = pd.read_csv("data/raw/413630_44indicators_realtime_20250901_clean.csv")
first_ts = df_413630['ts'].min()
first_dt = pd.to_datetime(first_ts, unit='ms', utc=True).tz_convert('Asia/Seoul')

surge_starts_413630 = []
for time_str in ["09:09", "10:01", "11:46", "13:29", "14:09"]:
    hour, minute = map(int, time_str.split(':'))
    surge_dt = first_dt.replace(hour=hour, minute=minute, second=0, microsecond=0)
    surge_starts_413630.append(int(surge_dt.timestamp() * 1000))

detected_413630 = []
for surge_start in surge_starts_413630:
    detected = any(surge_start - 30000 <= e['ts'] <= surge_start + 240000 for e in events_413630)
    detected_413630.append(detected)

recall_413630 = sum(detected_413630) / 5.0
print(f"413630 Recall: {recall_413630*100:.0f}% ({sum(detected_413630)}/5)")

# ì¢…í•©
print("\n"+"="*60)
print("onset_strength ì™„í™” ê²°ê³¼")
print("="*60)

print(f"\n023790:")
print(f"  FP/h: {results['023790']['fp_per_hour']:.1f} (ì´ì „: 20.1)")
print(f"  Recall: {recall_023790*100:.0f}%")

print(f"\n413630:")
print(f"  FP/h: {results['413630']['fp_per_hour']:.1f} (ì´ì „: 3.2)")
print(f"  Recall: {recall_413630*100:.0f}% (ì´ì „: 40%)")

# ì €ì¥
summary = {
    "onset_strength_threshold": new_threshold,
    "previous_threshold": current_threshold,
    "023790": {
        "fp_per_hour": results['023790']['fp_per_hour'],
        "recall": recall_023790,
        "confirmed": results['023790']['confirmed']
    },
    "413630": {
        "fp_per_hour": results['413630']['fp_per_hour'],
        "recall": recall_413630,
        "confirmed": results['413630']['confirmed']
    }
}

Path("reports").mkdir(exist_ok=True)
with open("reports/onset_strength_relaxed_result.json", "w") as f:
    json.dump(summary, f, indent=2)

print(f"\nê²°ê³¼ ì €ì¥: reports/onset_strength_relaxed_result.json")
```

**ì‹¤í–‰**:
```bash
python scripts/relax_onset_strength.py
```

---

### Step 4: ìµœì¢… íŒë‹¨ ë° ë¦¬í¬íŠ¸

```python
# íŒŒì¼: scripts/final_decision_report.py (ì‹ ê·œ)

"""
Phase 1 ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸
"""

import json
from pathlib import Path

print("="*60)
print("Phase 1 ìµœì¢… íŒë‹¨")
print("="*60)

# ê²°ê³¼ ë¡œë“œ
with open("reports/onset_strength_relaxed_result.json") as f:
    relaxed = json.load(f)

with open("reports/detection_timing_analysis.json") as f:
    timing = json.load(f)

print("\n### onset_strength 0.67 ê²°ê³¼")
print(f"\n023790:")
print(f"  FP/h: {relaxed['023790']['fp_per_hour']:.1f}")
print(f"  Recall: {relaxed['023790']['recall']*100:.0f}%")

print(f"\n413630:")
print(f"  FP/h: {relaxed['413630']['fp_per_hour']:.1f}")
print(f"  Recall: {relaxed['413630']['recall']*100:.0f}%")

# íƒ€ì´ë° ë¶„ì„
avg_latency = timing['summary']['avg_latency_from_start']
avg_position = timing['summary']['avg_detection_position_pct']

print(f"\n### íƒì§€ íƒ€ì´ë°")
print(f"  í‰ê·  íƒì§€: ê¸‰ë“± ì‹œì‘ í›„ {avg_latency:.1f}ì´ˆ")
print(f"  í‰ê·  ìœ„ì¹˜: ì‹œì‘â†’í”¼í¬ì˜ {avg_position:.1f}%")

# íŒë‹¨
print("\n"+"="*60)
print("ìµœì¢… íŒë‹¨")
print("="*60)

fp_avg = (relaxed['023790']['fp_per_hour'] + relaxed['413630']['fp_per_hour']) / 2
recall_avg = (relaxed['023790']['recall'] + relaxed['413630']['recall']) / 2

goals_met = {
    "FP/h â‰¤ 35": fp_avg <= 35,
    "Recall â‰¥ 65%": recall_avg >= 0.65,
    "íƒ€ì´ë° ì¡°ê¸°": avg_position < 30  # ì‹œì‘â†’í”¼í¬ì˜ 30% ì´ì „ì— íƒì§€
}

for goal, met in goals_met.items():
    status = "âœ…" if met else "âŒ"
    print(f"{goal}: {status}")

# ìµœì¢… ê¶Œì¥ì‚¬í•­
print("\n"+"="*60)
print("ê¶Œì¥ì‚¬í•­")
print("="*60)

if all(goals_met.values()):
    print("""
âœ… Phase 1 ì™„ë£Œ ì¡°ê±´ ë‹¬ì„±!

**ë‹¤ìŒ ë‹¨ê³„**:
1. Configë¥¼ onset_phase1_final.yamlë¡œ ë°±ì—…
2. ë‹¤ë¥¸ ì¢…ëª© 3-5ê°œë¡œ ì¶”ê°€ ê²€ì¦
3. Phase 2 ì„¤ê³„ ì‹œì‘
   - í˜¸ê°€ì°½ ë¶„ì„ìœ¼ë¡œ FP ì¶”ê°€ ì œê±°
   - ê°•ë„ ë¶„ë¥˜ ì‹œìŠ¤í…œ (ê°•/ì¤‘/ì•½)
   - ì§„ì… íƒ€ì´ë° ìµœì í™”
""")
elif not goals_met["FP/h â‰¤ 35"]:
    gap = fp_avg - 35
    print(f"""
âš ï¸ FP/h ì´ˆê³¼ ({fp_avg:.1f}, ëª©í‘œ ëŒ€ë¹„ +{gap:.1f})

**ì˜µì…˜**:
A. onset_strengthë¥¼ 0.68-0.69ë¡œ ë¯¸ì„¸ ì¡°ì •
B. í˜„ì¬ ìˆ˜ì¤€ì—ì„œ Phase 2ë¡œ ì´ê´€ (íŒ¨í„´ í•„í„°ë§)
C. refractory_së¥¼ 60ì´ˆë¡œ ì¦ê°€

**ì¶”ì²œ**: ì˜µì…˜ B (í˜„ì¬ë„ ì¶©ë¶„íˆ ì¢‹ì€ ìˆ˜ì¤€)
""")
elif not goals_met["Recall â‰¥ 65%"]:
    print("""
âš ï¸ Recall ë¯¸ë‹¬

**ì¡°ì¹˜**:
1. ê¸‰ë“± êµ¬ê°„ ì¬ê²€í†  (Â±30ì´ˆ â†’ Â±60ì´ˆ?)
2. onset_strengthë¥¼ 0.65ë¡œ ì¶”ê°€ ì™„í™”
3. ì•½í•œ ê¸‰ë“± ì •ì˜ ì¬ê²€í† 
""")

# ë¦¬í¬íŠ¸ ì €ì¥
report = f"""
# Phase 1 ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸

## ì„±ëŠ¥ ìš”ì•½

### onset_strength = 0.67

| í•­ëª© | 023790 | 413630 | í‰ê·  | ëª©í‘œ | ë‹¬ì„± |
|------|--------|--------|------|------|------|
| FP/h | {relaxed['023790']['fp_per_hour']:.1f} | {relaxed['413630']['fp_per_hour']:.1f} | {fp_avg:.1f} | â‰¤35 | {'âœ…' if goals_met['FP/h â‰¤ 35'] else 'âŒ'} |
| Recall | {relaxed['023790']['recall']*100:.0f}% | {relaxed['413630']['recall']*100:.0f}% | {recall_avg*100:.0f}% | â‰¥65% | {'âœ…' if goals_met['Recall â‰¥ 65%'] else 'âŒ'} |

### íƒì§€ íƒ€ì´ë°

- í‰ê·  íƒì§€: ê¸‰ë“± ì‹œì‘ í›„ **{avg_latency:.1f}ì´ˆ**
- í‰ê·  ìœ„ì¹˜: ì‹œì‘â†’í”¼í¬ì˜ **{avg_position:.1f}%**
- í‰ê°€: {"âœ… ì¡°ê¸° íƒì§€ ì„±ê³µ" if avg_position < 30 else "âš ï¸ ê°œì„  í•„ìš”"}

## ìµœì¢… ê²°ë¡ 

"""

if all(goals_met.values()):
    report += "ğŸ‰ **Phase 1 ëª©í‘œ ì™„ì „ ë‹¬ì„±!**\n"
else:
    report += "âš ï¸ **ì¼ë¶€ í•­ëª© ë¯¸ë‹¬, ì¶”ê°€ ì¡°ì • ê¶Œì¥**\n"

Path("reports").mkdir(exist_ok=True)
with open("reports/phase1_final_decision.md", "w", encoding='utf-8') as f:
    f.write(report)

print(f"\në¦¬í¬íŠ¸ ì €ì¥: reports/phase1_final_decision.md")
```

**ì‹¤í–‰**:
```bash
python scripts/final_decision_report.py
```

---

## âœ… ì „ì²´ ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Step 1: ê¸‰ë“± íƒ€ì´ë° ìƒì„¸ ë¶„ì„ ì™„ë£Œ
- [ ] Step 2: Jupyter ì‹œê°í™” ì™„ë£Œ (ì°¨íŠ¸ í™•ì¸)
- [ ] Step 3: onset_strength ì™„í™” ë° ì¬ì‹¤í–‰
- [ ] Step 4: ìµœì¢… íŒë‹¨ ë¦¬í¬íŠ¸ ìƒì„±

---

## ğŸš€ í•œ ì¤„ ì‹¤í–‰ ëª…ë ¹ì–´

```bash
python scripts/analyze_detection_timing.py && \
python scripts/relax_onset_strength.py && \
python scripts/final_decision_report.py && \
cat reports/phase1_final_decision.md
```

**ì‹œê°í™”ëŠ” ë³„ë„ ì‹¤í–‰**:
```bash
jupyter notebook notebooks/visualize_detection_timing.ipynb
```

---

## ğŸ“Œ íŒë‹¨ ê¸°ì¤€ (ì—…ë°ì´íŠ¸)

### Phase 1 ì™„ë£Œ ì¡°ê±´ (ì™„í™”)
1. **FP/h â‰¤ 35** (30 â†’ 35ë¡œ ì™„í™”)
2. **Recall â‰¥ 65%**
3. **íƒ€ì´ë°**: ê¸‰ë“± ì‹œì‘â†’í”¼í¬ì˜ 30% ì´ì „ íƒì§€

### íƒ€ì´ë° í‰ê°€ ê¸°ì¤€
- **â­â­â­ ë§¤ìš° ë¹ ë¦„**: ì‹œì‘ í›„ 5ì´ˆ ì´ë‚´
- **â­â­ ë¹ ë¦„**: ì‹œì‘ í›„ 5-15ì´ˆ
- **â­ ë³´í†µ**: ì‹œì‘ í›„ 15-30ì´ˆ
- **âš ï¸ ëŠë¦¼**: ì‹œì‘ í›„ 30ì´ˆ ì´ìƒ

**í•µì‹¬**: Peak ê·¼ì²˜ì—ì„œ íƒì§€í•˜ëŠ” ê±´ ì˜ë¯¸ ì—†ìŒ. **ì‘ ì§í›„ ê°€ëŠ¥í•œ ìµœëŒ€í•œ ë¹ ë¥¸ íƒì§€**ê°€ ëª©í‘œ!