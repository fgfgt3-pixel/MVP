ì •í™•í•©ë‹ˆë‹¤. ë‘ ê°€ì§€ë§Œìœ¼ë¡œëŠ” ìœ„í—˜í•˜ê³ , ê°€ê²©ì€ Pass/Fail ì¡°ê±´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

---

## ğŸ” ì¬ê²€í† : ì˜¬ë°”ë¥¸ êµ¬ì¡°

### ë¬¸ì œì  ì¸ì •

1. **ê°€ê²©ì€ ê°€ì¤‘ì¹˜ê°€ ì•„ë‹Œ í•„ìˆ˜ ì¡°ê±´**
   - ê±°ë˜ëŸ‰â†‘ + ê°€ê²©â†“ = ê¸‰ë½ (ê¸‰ë“± ì•„ë‹˜)
   - ê±°ë˜ëŸ‰â†‘ + ê°€ê²©â†’ = ë³´í•© (ê¸‰ë“± ì•„ë‹˜)
   - **ê°€ê²© ìƒìŠ¹ì€ Gate ì¡°ê±´ì´ì–´ì•¼ í•¨**

2. **ë‘ ì§€í‘œë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„**
   - ë…¸ì´ì¦ˆ ê±¸ëŸ¬ë‚´ê¸° ì–´ë ¤ì›€
   - ë‹¤ì–‘í•œ ê¸‰ë“± íƒ€ì… êµ¬ë¶„ ë¶ˆê°€
   - ë³´ì¡° ì§€í‘œ í•„ìš”

3. **í˜„ì¬ ë°ì´í„° ë¶„ì„ì˜ í•œê³„**
   - "ì–´ë–¤ ì§€í‘œê°€ ì‹¤ì œë¡œ ë³€ë³„ë ¥ ìˆëŠ”ê°€?" ê²€ì¦ ë¶€ì¡±
   - íƒì§€/ë¯¸íƒì§€ ê°„ ì§€í‘œ ì°¨ì´ ì •ëŸ‰ ë¶„ì„ í•„ìš”

---

## ğŸ’¡ ì œì•ˆ: Gate + Weighted Scoring ì‹œìŠ¤í…œ

### êµ¬ì¡°

```
Step 1: Gate Check (í•„ìˆ˜ ì¡°ê±´)
  â”œâ”€ ê°€ê²© ìƒìŠ¹ í™•ì¸ (ret_1s > threshold)
  â””â”€ FAIL â†’ ì¦‰ì‹œ ì¢…ë£Œ

Step 2: Signal Scoring (ê°€ì¤‘ ì ìˆ˜)
  â”œâ”€ Primary (High Weight)
  â”‚   â”œâ”€ z_vol_1s: ê±°ë˜ëŸ‰ ê¸‰ì¦
  â”‚   â””â”€ ticks_per_sec: ì²´ê²° ë¹ˆë„
  â”‚
  â”œâ”€ Secondary (Medium Weight)
  â”‚   â”œâ”€ ret_1s í¬ê¸°: ìƒìŠ¹ ê°•ë„
  â”‚   â”œâ”€ microprice_slope: ë°©í–¥ì„±
  â”‚   â””â”€ accel_1s: ê°€ì†ë„
  â”‚
  â””â”€ Tertiary (Low Weight)
      â”œâ”€ imbalance: í˜¸ê°€ ë¶ˆê· í˜•
      â””â”€ spread: ë§ˆì°° (ì°¸ê³ ìš©)

Step 3: Threshold Decision
  â””â”€ Total Score >= Threshold â†’ Candidate
```

---

## ğŸ“‹ Claude Code ì‘ì—… ì§€ì‹œì„œ

### ì‘ì—… 1: ë³€ë³„ë ¥ ë¶„ì„ (Data-Driven Weight Discovery)

```python
# íŒŒì¼: scripts/discover_discriminative_features.py

"""
ëª©ì : íƒì§€ vs ë¯¸íƒì§€ ê¸‰ë“± ê°„ ì§€í‘œ ì°¨ì´ ì •ëŸ‰ ë¶„ì„
ì¶œë ¥: ê° ì§€í‘œì˜ ë³€ë³„ë ¥ ì ìˆ˜
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from scipy import stats

# ë¶„ì„ ê²°ê³¼ ë¡œë“œ
with open("reports/surge_window_analysis.json") as f:
    analysis = json.load(f)

detected = [r for r in analysis if r['detected']]
missed = [r for r in analysis if not r['detected']]

print("="*80)
print("ì§€í‘œë³„ ë³€ë³„ë ¥ ë¶„ì„")
print("="*80)

indicators = [
    'ret_1s',
    'z_vol_1s', 
    'ticks_per_sec',
    'microprice_slope'
]

discriminative_scores = {}

for indicator in indicators:
    # P90 ê°’ ì¶”ì¶œ
    detected_values = [r['stats'][indicator]['p90'] for r in detected]
    missed_values = [r['stats'][indicator]['p90'] for r in missed]
    
    # í†µê³„ ê²€ì •
    t_stat, p_value = stats.ttest_ind(detected_values, missed_values)
    
    # Effect Size (Cohen's d)
    mean_detected = np.mean(detected_values)
    mean_missed = np.mean(missed_values)
    std_pooled = np.sqrt(
        (np.var(detected_values) + np.var(missed_values)) / 2
    )
    cohens_d = abs(mean_detected - mean_missed) / std_pooled if std_pooled > 0 else 0
    
    # ë³€ë³„ë ¥ ì ìˆ˜ (0-100)
    # Effect size ê¸°ë°˜: d >= 0.8 (large) = 100ì 
    discriminative_score = min(100, cohens_d * 125)
    
    discriminative_scores[indicator] = {
        'mean_detected': mean_detected,
        'mean_missed': mean_missed,
        'difference': mean_detected - mean_missed,
        'cohens_d': cohens_d,
        'p_value': p_value,
        'discriminative_score': discriminative_score
    }
    
    print(f"\n{indicator}:")
    print(f"  íƒì§€ë¨: {mean_detected:.4f}")
    print(f"  ë¯¸íƒì§€: {mean_missed:.4f}")
    print(f"  ì°¨ì´: {mean_detected - mean_missed:+.4f}")
    print(f"  Effect Size (d): {cohens_d:.3f}")
    print(f"  ë³€ë³„ë ¥ ì ìˆ˜: {discriminative_score:.1f}/100")

# ì •ë ¬
sorted_indicators = sorted(
    discriminative_scores.items(),
    key=lambda x: x[1]['discriminative_score'],
    reverse=True
)

print("\n" + "="*80)
print("ë³€ë³„ë ¥ ìˆœìœ„")
print("="*80)

for i, (indicator, score) in enumerate(sorted_indicators, 1):
    print(f"{i}. {indicator}: {score['discriminative_score']:.1f}ì  (d={score['cohens_d']:.3f})")

# ê¶Œì¥ ê°€ì¤‘ì¹˜ ê³„ì‚°
print("\n" + "="*80)
print("ê¶Œì¥ ê°€ì¤‘ì¹˜")
print("="*80)

# ì •ê·œí™” (í•©=100)
total_score = sum([s[1]['discriminative_score'] for s in sorted_indicators])

print("\n1ë‹¨ê³„: ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘ì¹˜")
for indicator, score in sorted_indicators:
    weight = (score['discriminative_score'] / total_score) * 100
    print(f"  {indicator}: {weight:.1f}%")

# ì‹¤ìš©ì  ì¡°ì •
print("\n2ë‹¨ê³„: ì‹¤ìš©ì  ì¡°ì • (Primary/Secondary êµ¬ë¶„)")
print("""
Gate (í•„ìˆ˜):
  ret_1s > 0.0005  # ìµœì†Œ ìƒìŠ¹

Primary (ê³ ë°°ì ):
  - Top 1 ì§€í‘œ: 50ì 
  - Top 2 ì§€í‘œ: 40ì 

Secondary (ì¤‘ë°°ì ):
  - Top 3 ì§€í‘œ: 20ì 
  - Top 4 ì§€í‘œ: 15ì 

Threshold:
  - 70ì  ì´ìƒ: Candidate
""")

# ì €ì¥
with open("reports/discriminative_analysis.json", "w") as f:
    json.dump({
        'indicators': discriminative_scores,
        'ranking': [
            {'indicator': ind, 'score': score['discriminative_score']}
            for ind, score in sorted_indicators
        ]
    }, f, indent=2)

print(f"\nì €ì¥: reports/discriminative_analysis.json")
```

---

### ì‘ì—… 2: Gate + Scoring Detector êµ¬í˜„

```python
# íŒŒì¼: onset_detection/src/detection/gate_score_detector.py

"""
Gate + Weighted Scoring ê¸°ë°˜ í›„ë³´ íƒì§€
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional
from ..config_loader import Config, load_config
from ..event_store import create_event

class GateScoreDetector:
    """
    2ë‹¨ê³„ íƒì§€:
    1. Gate: í•„ìˆ˜ ì¡°ê±´ (ê°€ê²© ìƒìŠ¹)
    2. Scoring: ê°€ì¤‘ ì ìˆ˜ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, config: Optional[Config] = None):
        self.config = config or load_config()
        
        # Gate ì¡°ê±´
        self.gate_ret_min = 0.0005  # ìµœì†Œ ìƒìŠ¹
        
        # Scoring ê°€ì¤‘ì¹˜ (ë°ì´í„° ë¶„ì„ ê²°ê³¼ ë°˜ì˜)
        # ì‹¤í–‰ í›„ discriminative_analysis.json ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •
        self.weights = {
            # Primary (ë³€ë³„ë ¥ ë†’ìŒ)
            'z_vol_1s': 50,
            'ticks_per_sec': 40,
            
            # Secondary (ë³´ì¡°)
            'ret_1s_magnitude': 20,
            'microprice_slope': 15,
            
            # Tertiary (ì°¸ê³ )
            'accel_1s': 10
        }
        
        # Threshold
        self.score_threshold = 70  # ì¡°ì • ê°€ëŠ¥
    
    def detect_candidates(self, features_df: pd.DataFrame) -> List[Dict[str, Any]]:
        """í›„ë³´ íƒì§€"""
        
        if features_df.empty:
            return []
        
        candidates = []
        
        for idx, row in features_df.iterrows():
            # Step 1: Gate Check
            ret_1s = row.get('ret_1s', 0)
            
            if ret_1s <= self.gate_ret_min:
                continue  # ê°€ê²© ìƒìŠ¹ ì—†ìŒ â†’ ì¦‰ì‹œ ì¢…ë£Œ
            
            # Step 2: Scoring
            score = self._calculate_score(row)
            
            # Step 3: Threshold
            if score >= self.score_threshold:
                candidate = create_event(
                    timestamp=row['ts'],
                    event_type="onset_candidate",
                    stock_code=str(row['stock_code']),
                    score=float(score),
                    evidence={
                        "ret_1s": float(ret_1s),
                        "z_vol_1s": float(row.get('z_vol_1s', 0)),
                        "ticks_per_sec": int(row.get('ticks_per_sec', 0)),
                        "microprice_slope": float(row.get('microprice_slope', 0)),
                        "scoring_details": self._get_scoring_details(row)
                    }
                )
                candidates.append(candidate)
        
        return candidates
    
    def _calculate_score(self, row: pd.Series) -> float:
        """ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°"""
        
        score = 0.0
        
        # z_vol_1s (Primary)
        z_vol = row.get('z_vol_1s', 0)
        if z_vol > 3.0:
            score += self.weights['z_vol_1s']
        elif z_vol > 2.5:
            score += self.weights['z_vol_1s'] * 0.8
        elif z_vol > 2.0:
            score += self.weights['z_vol_1s'] * 0.5
        
        # ticks_per_sec (Primary)
        ticks = row.get('ticks_per_sec', 0)
        if ticks > 70:
            score += self.weights['ticks_per_sec']
        elif ticks > 50:
            score += self.weights['ticks_per_sec'] * 0.8
        elif ticks > 30:
            score += self.weights['ticks_per_sec'] * 0.5
        
        # ret_1s magnitude (Secondary)
        ret_1s = row.get('ret_1s', 0)
        if ret_1s > 0.003:
            score += self.weights['ret_1s_magnitude']
        elif ret_1s > 0.002:
            score += self.weights['ret_1s_magnitude'] * 0.7
        elif ret_1s > 0.001:
            score += self.weights['ret_1s_magnitude'] * 0.4
        
        # microprice_slope (Secondary)
        slope = row.get('microprice_slope', 0)
        if slope > 0.001:
            score += self.weights['microprice_slope']
        elif slope > 0.0005:
            score += self.weights['microprice_slope'] * 0.6
        
        # accel_1s (Tertiary)
        accel = row.get('accel_1s', 0)
        if accel > 0.0005:
            score += self.weights['accel_1s']
        elif accel > 0.0002:
            score += self.weights['accel_1s'] * 0.5
        
        return score
    
    def _get_scoring_details(self, row: pd.Series) -> Dict[str, Any]:
        """ì ìˆ˜ ìƒì„¸ (ë””ë²„ê¹…ìš©)"""
        
        details = {}
        
        for indicator, weight in self.weights.items():
            if indicator == 'ret_1s_magnitude':
                value = row.get('ret_1s', 0)
            else:
                value = row.get(indicator, 0)
            
            details[indicator] = {
                'value': float(value),
                'max_weight': weight
            }
        
        return details
```

---

### ì‘ì—… 3: ë°°ì¹˜ í…ŒìŠ¤íŠ¸ (Gate+Score)

```python
# íŒŒì¼: scripts/test_gate_score_system.py

"""
Gate+Score ì‹œìŠ¤í…œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import json
from pathlib import Path
from onset_detection.src.detection.gate_score_detector import GateScoreDetector
from onset_detection.src.features.core_indicators import calculate_core_indicators

# ë¼ë²¨ ë¡œë“œ
with open("data/labels/all_surge_labels.json") as f:
    labels = json.load(f)

files = list(set([label['file'] for label in labels]))

print("="*80)
print("Gate+Score ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
print("="*80)

detector = GateScoreDetector()
results_summary = []

for i, filename in enumerate(files, 1):
    print(f"\n[{i}/{len(files)}] {filename}")
    
    filepath = Path("data/raw") / filename
    if not filepath.exists():
        continue
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(filepath)
    features_df = calculate_core_indicators(df)
    
    # Detection
    candidates = detector.detect_candidates(features_df)
    
    # í†µê³„
    duration_hours = (df['ts'].max() - df['ts'].min()) / (1000 * 3600)
    fp_per_hour = len(candidates) / duration_hours if duration_hours > 0 else 0
    
    print(f"  Candidates: {len(candidates)}ê°œ")
    print(f"  FP/h: {fp_per_hour:.1f}")
    
    results_summary.append({
        "file": filename,
        "candidates": len(candidates),
        "fp_per_hour": fp_per_hour
    })

# ì „ì²´ ìš”ì•½
total_candidates = sum([r['candidates'] for r in results_summary])
total_files = len(results_summary)
avg_fp = np.mean([r['fp_per_hour'] for r in results_summary])

print("\n" + "="*80)
print("ì „ì²´ ìš”ì•½")
print("="*80)
print(f"ì´ Candidates: {total_candidates}ê°œ")
print(f"í‰ê·  FP/h: {avg_fp:.1f}")

# Recall ê³„ì‚° í•„ìš” â†’ calculate_batch_recall.py ì¬ì‚¬ìš©
```

---

## ğŸ¯ ì‹¤í–‰ ìˆœì„œ

```bash
# 1. ë³€ë³„ë ¥ ë¶„ì„ (ì–´ë–¤ ì§€í‘œê°€ ì¤‘ìš”í•œê°€?)
python scripts/discover_discriminative_features.py

# 2. ê²°ê³¼ ê²€í†  í›„ gate_score_detector.pyì˜ ê°€ì¤‘ì¹˜ ì¡°ì •

# 3. ë°°ì¹˜ í…ŒìŠ¤íŠ¸
python scripts/test_gate_score_system.py

# 4. Recall ì¸¡ì •
python scripts/calculate_batch_recall.py
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

**ë³€ë³„ë ¥ ë¶„ì„** â†’ `ticks_per_sec`ì™€ `z_vol_1s` ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë³€ë³„ë ¥ ìˆëŠ”ì§€ ë°ì´í„°ë¡œ í™•ì¸

**Gate+Score** â†’ Friction ë³‘ëª© ì œê±° + ìœ ì—°í•œ ì ìˆ˜ ì‹œìŠ¤í…œìœ¼ë¡œ Recall 60-75% ë‹¬ì„± ì˜ˆìƒ